{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81hPUOIugpPR",
        "outputId": "efdfb487-d9f2-44d8-93c2-9668088bf50c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using project directory: /vol/bitbucket/ag724/individual_project/cah/personalized-humour-generation/cah-app\n",
            "/vol/bitbucket/ag724/individual_project/cah/personalized-humour-generation/cah-app\n"
          ]
        }
      ],
      "source": [
        "PROJECT_DIR = \"/vol/bitbucket/ag724/individual_project/cah/personalized-humour-generation/python-backend\"\n",
        "print(f\"Using project directory: {PROJECT_DIR}\")\n",
        "import os\n",
        "os.chdir(PROJECT_DIR)\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zqH5KsVbUT6G",
        "outputId": "cae2c596-dae9-415f-dbf8-94debe8ebfdc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (2.7.0+cu118)\n",
            "Requirement already satisfied: torchvision in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (0.22.0+cu118)\n",
            "Requirement already satisfied: torchaudio in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (2.7.0+cu118)\n",
            "Requirement already satisfied: filelock in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: setuptools in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch) (80.3.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch) (11.8.86)\n",
            "Requirement already satisfied: triton==3.3.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch) (3.3.0)\n",
            "Requirement already satisfied: numpy in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torchvision) (2.1.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: transformers==4.38.1 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (4.38.1)\n",
            "Requirement already satisfied: datasets in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (3.6.0)\n",
            "Requirement already satisfied: accelerate in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (1.6.0)\n",
            "Requirement already satisfied: evaluate in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (0.4.3)\n",
            "Requirement already satisfied: peft in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (0.15.2)\n",
            "Requirement already satisfied: bitsandbytes in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (0.45.5)\n",
            "Requirement already satisfied: filelock in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from transformers==4.38.1) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from transformers==4.38.1) (0.31.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from transformers==4.38.1) (2.1.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from transformers==4.38.1) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from transformers==4.38.1) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from transformers==4.38.1) (2024.11.6)\n",
            "Requirement already satisfied: requests in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from transformers==4.38.1) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from transformers==4.38.1) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from transformers==4.38.1) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from transformers==4.38.1) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from datasets) (20.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
            "Requirement already satisfied: xxhash in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: psutil in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from accelerate) (7.0.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from accelerate) (2.7.0+cu118)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.1) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.1) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from requests->transformers==4.38.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from requests->transformers==4.38.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from requests->transformers==4.38.1) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from requests->transformers==4.38.1) (2025.4.26)\n",
            "Requirement already satisfied: setuptools in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (80.3.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
            "Requirement already satisfied: networkx in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.8.86)\n",
            "Requirement already satisfied: triton==3.3.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: sentencepiece in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (0.2.0)\n",
            "Requirement already satisfied: scikit-learn in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (2.2.3)\n",
            "Requirement already satisfied: tqdm in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (4.67.1)\n",
            "Requirement already satisfied: wandb in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (0.19.11)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from scikit-learn) (2.1.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from wandb) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from wandb) (6.30.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from wandb) (7.0.0)\n",
            "Requirement already satisfied: pydantic<3 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from wandb) (2.6.3)\n",
            "Requirement already satisfied: pyyaml in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from wandb) (2.27.0)\n",
            "Requirement already satisfied: setproctitle in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: setuptools in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from wandb) (80.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from wandb) (4.13.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from pydantic<3->wandb) (2.16.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: fastapi==0.110.1 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (0.110.1)\n",
            "Requirement already satisfied: uvicorn==0.30.1 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (0.30.1)\n",
            "Requirement already satisfied: pydantic==2.6.3 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (2.6.3)\n",
            "Requirement already satisfied: python-dotenv==1.0.1 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (1.0.1)\n",
            "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from fastapi==0.110.1) (0.37.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from fastapi==0.110.1) (4.13.2)\n",
            "Requirement already satisfied: click>=7.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from uvicorn==0.30.1) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from uvicorn==0.30.1) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from pydantic==2.6.3) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from pydantic==2.6.3) (2.16.3)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from starlette<0.38.0,>=0.37.2->fastapi==0.110.1) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from anyio<5,>=3.4.0->starlette<0.38.0,>=0.37.2->fastapi==0.110.1) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from anyio<5,>=3.4.0->starlette<0.38.0,>=0.37.2->fastapi==0.110.1) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install transformers==4.38.1 datasets accelerate evaluate peft bitsandbytes\n",
        "!pip install sentencepiece scikit-learn pandas tqdm wandb\n",
        "!pip install fastapi==0.110.1 uvicorn==0.30.1 pydantic==2.6.3 python-dotenv==1.0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2yZ6CjEef4Z",
        "outputId": "342c151c-95c2-403f-fd42-56e95a344cc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU available: True\n",
            "GPU device: NVIDIA A30\n",
            "Using project directory: /vol/bitbucket/ag724/individual_project/cah/personalized-humour-generation/cah-app\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Verify GPU availability\n",
        "import torch\n",
        "print('GPU available:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU device:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# Set up project directory\n",
        "import os\n",
        "PROJECT_DIR = os.getcwd()\n",
        "print(f\"Using project directory: {PROJECT_DIR}\")\n",
        "\n",
        "# Create necessary directories if they don't exist\n",
        "!mkdir -p data/{raw,processed,debug}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_WPsfm6gtN5",
        "outputId": "97ffb40a-26a6-4252-a758-cea1acd88d4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing CAH data...\n",
            "Reading JSON file...\n",
            "Processing cards...\n",
            "\rProcessing white cards:   0% 0/22441 [00:00<?, ?it/s]\rProcessing white cards: 100% 22441/22441 [00:00<00:00, 2418654.95it/s]\n",
            "\rProcessing black cards:   0% 0/6308 [00:00<?, ?it/s]\rProcessing black cards: 100% 6308/6308 [00:00<00:00, 1800576.40it/s]\n",
            "Creating DataFrame...\n",
            "Splitting data...\n",
            "Saving processed data...\n",
            "\n",
            "Processing complete!\n",
            "Processed 28748 unique cards\n",
            "Black cards: 6308\n",
            "White cards: 22440\n",
            "Train: 24579, Validation: 1294, Test: 2875\n",
            "\n",
            "Example black cards:\n",
            "22441          _ ? there is medication for that, you know.\n",
            "22442               _ is a slippery slope that leads to _.\n",
            "22443                                       _ It's a trap!\n",
            "22444    _ really helped my dad through his midlife cri...\n",
            "22445                 _ will finally put an end to racism.\n",
            "\n",
            "Example white cards:\n",
            "0                                                69\n",
            "1                                               420\n",
            "2                                              1790\n",
            "3                                 ...it is unclear.\n",
            "4    ...then the penis is inserted into the vagina.\n"
          ]
        }
      ],
      "source": [
        "# Run the data processing script\n",
        "!python src/download_data.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEfnf6WLg-vw",
        "outputId": "a4167c1b-fbeb-4558-eecf-41ac841e91c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set size: 24579\n",
            "Validation set size: 1294\n",
            "Test set size: 2875\n",
            "\n",
            "Sample training examples:\n",
            "      card_type                                               text pack  pick\n",
            "25268     black   In a fight to the death against _ I would use _.  all   2.0\n",
            "20275     white                              The walls of Jericho.  all   NaN\n",
            "10161     white                   Going to the movies by yourself.  all   NaN\n",
            "2952      white           a sort of whisker, or rather a moustache  all   NaN\n",
            "28011     black  What the hell?! They added a 6/6 with flying, ...  all   1.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Define the data directory\n",
        "data_dir = Path('data/processed')\n",
        "\n",
        "# Load the data\n",
        "train_df = pd.read_parquet(data_dir / 'cah_train.parquet')\n",
        "valid_df = pd.read_parquet(data_dir / 'cah_valid.parquet')\n",
        "test_df = pd.read_parquet(data_dir / 'cah_test.parquet')\n",
        "\n",
        "# Print the sizes of the datasets\n",
        "print(f\"Train set size: {len(train_df)}\")\n",
        "print(f\"Validation set size: {len(valid_df)}\")\n",
        "print(f\"Test set size: {len(test_df)}\")\n",
        "\n",
        "# Display some examples from the training set\n",
        "print(\"\\nSample training examples:\")\n",
        "print(train_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAf8r4eSkkkH",
        "outputId": "6a7f85c3-ae33-43ba-a1ed-46ea61e19aea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.10.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting seaborn\n",
            "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting wordcloud\n",
            "  Downloading wordcloud-1.9.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting nltk\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib)\n",
            "  Downloading contourpy-1.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib)\n",
            "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib)\n",
            "  Downloading fonttools-4.57.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (102 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.5/102.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib)\n",
            "  Downloading kiwisolver-1.4.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: numpy>=1.23 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from matplotlib) (2.1.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from matplotlib) (11.0.0)\n",
            "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
            "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pandas>=1.2 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from seaborn) (2.2.3)\n",
            "Requirement already satisfied: click in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from nltk) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Downloading matplotlib-3.10.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[?25hDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wordcloud-1.9.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (539 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m539.2/539.2 kB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading contourpy-1.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (323 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.7/323.7 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading fonttools-4.57.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m162.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kiwisolver-1.4.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m111.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.1/111.1 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyparsing, nltk, kiwisolver, fonttools, cycler, contourpy, matplotlib, wordcloud, seaborn\n",
            "Successfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.57.0 kiwisolver-1.4.8 matplotlib-3.10.1 nltk-3.9.1 pyparsing-3.2.3 seaborn-0.13.2 wordcloud-1.9.4\n"
          ]
        }
      ],
      "source": [
        "!pip install matplotlib seaborn wordcloud nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVG-H1YYk5N_",
        "outputId": "713a0b3c-b595-4f47-d8e6-26ab975f9880"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "Dataset Statistics:\n",
            "Total cards: 28748\n",
            "Black cards: 5393\n",
            "White cards: 19186\n",
            "\n",
            "Card Length Statistics:\n",
            "             count       mean        std  min   25%   50%   75%    max\n",
            "card_type                                                             \n",
            "black       5393.0  54.082885  26.683258  2.0  36.0  50.0  68.0  908.0\n",
            "white      19186.0  29.787293  18.226663  2.0  16.0  26.0  39.0  249.0\n",
            "\n",
            "Processing black cards...\n",
            "Processing white cards...\n",
            "\n",
            "Most Common Words in Black Cards:\n",
            "[('like', 231), ('new', 204), ('name', 177), ('would', 167), ('get', 165), ('insert', 147), ('time', 140), ('one', 132), ('never', 128), ('trump', 125), ('always', 121), ('got', 117), ('best', 115), ('first', 114), ('want', 111), ('thing', 108), ('love', 103), ('night', 99), ('last', 98), ('life', 95)]\n",
            "\n",
            "Most Common Words in White Cards:\n",
            "[('getting', 319), ('sex', 268), ('like', 232), ('one', 213), ('baby', 190), ('man', 172), ('people', 170), ('shit', 152), ('hot', 147), ('dick', 138), ('old', 138), ('time', 138), ('white', 135), ('big', 132), ('eating', 131), ('little', 127), ('back', 127), ('going', 125), ('fucking', 123), ('day', 119)]\n",
            "\n",
            "Blank Count Statistics:\n",
            "blank_count\n",
            "0      696\n",
            "1     4186\n",
            "2      474\n",
            "3       34\n",
            "4        1\n",
            "10       2\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Lexical Diversity Analysis:\n",
            "Black cards lexical diversity: 0.122\n",
            "White cards lexical diversity: 0.165\n",
            "\n",
            "Sentiment Analysis:\n",
            "             count      mean       std  min  25%  50%      75%  max\n",
            "card_type                                                          \n",
            "black       5393.0  0.068529  0.293596 -1.0  0.0  0.0  0.15625  1.0\n",
            "white      19186.0  0.012067  0.225861 -1.0  0.0  0.0  0.00000  1.0\n",
            "\n",
            "N-gram Analysis:\n",
            "\n",
            "Top 5 2-grams in black cards:\n",
            "  of _: 436\n",
            "  with _: 264\n",
            "  for _: 238\n",
            "  i m: 225\n",
            "  about _: 220\n",
            "\n",
            "Top 5 3-grams in black cards:\n",
            "  _ and _: 101\n",
            "  what s the: 42\n",
            "  _ in the: 41\n",
            "  _ is the: 34\n",
            "  i don t: 33\n",
            "\n",
            "Top 5 2-grams in white cards:\n",
            "  with a: 366\n",
            "  in the: 361\n",
            "  of the: 281\n",
            "  in a: 208\n",
            "  on the: 175\n",
            "\n",
            "Top 5 3-grams in white cards:\n",
            "  the world s: 40\n",
            "  a sopping wet: 21\n",
            "  with a sopping: 20\n",
            "  some kind of: 20\n",
            "  out of the: 20\n",
            "\n",
            "Surprise Index Analysis:\n",
            "Black cards average surprise index: 0.000\n",
            "White cards average surprise index: 0.000\n",
            "\n",
            "Sample Card Combinations:\n",
            "\n",
            "Black Card: I bet you can't have just one _!\n",
            "  + The sweaty hand of a child = I bet you can't have just one The sweaty hand of a child!\n",
            "  + Cheesman Park: where you can bury a bone AND uncover one. = I bet you can't have just one Cheesman Park: where you can bury a bone AND uncover one.!\n",
            "  + Getting locked inside a museum = I bet you can't have just one Getting locked inside a museum!\n",
            "  + Asian girls with names like \"Christina\" or \"Elizabeth\". = I bet you can't have just one Asian girls with names like \"Christina\" or \"Elizabeth\".!\n",
            "  + Pretending golf is fun. = I bet you can't have just one Pretending golf is fun.!\n",
            "\n",
            "Black Card: My Mom ran over _ with the mini van.\n",
            "  + The sweaty hand of a child = My Mom ran over The sweaty hand of a child with the mini van.\n",
            "  + Cheesman Park: where you can bury a bone AND uncover one. = My Mom ran over Cheesman Park: where you can bury a bone AND uncover one. with the mini van.\n",
            "  + Getting locked inside a museum = My Mom ran over Getting locked inside a museum with the mini van.\n",
            "  + Asian girls with names like \"Christina\" or \"Elizabeth\". = My Mom ran over Asian girls with names like \"Christina\" or \"Elizabeth\". with the mini van.\n",
            "  + Pretending golf is fun. = My Mom ran over Pretending golf is fun. with the mini van.\n",
            "\n",
            "Black Card: What do you get the person who has everything?\n",
            "  + The sweaty hand of a child = What do you get the person who has everything?\n",
            "  + Cheesman Park: where you can bury a bone AND uncover one. = What do you get the person who has everything?\n",
            "  + Getting locked inside a museum = What do you get the person who has everything?\n",
            "  + Asian girls with names like \"Christina\" or \"Elizabeth\". = What do you get the person who has everything?\n",
            "  + Pretending golf is fun. = What do you get the person who has everything?\n",
            "\n",
            "Black Card: I'm out of coffee, so the only thing keepig me alive right now is _.\n",
            "  + The sweaty hand of a child = I'm out of coffee, so the only thing keepig me alive right now is The sweaty hand of a child.\n",
            "  + Cheesman Park: where you can bury a bone AND uncover one. = I'm out of coffee, so the only thing keepig me alive right now is Cheesman Park: where you can bury a bone AND uncover one..\n",
            "  + Getting locked inside a museum = I'm out of coffee, so the only thing keepig me alive right now is Getting locked inside a museum.\n",
            "  + Asian girls with names like \"Christina\" or \"Elizabeth\". = I'm out of coffee, so the only thing keepig me alive right now is Asian girls with names like \"Christina\" or \"Elizabeth\"..\n",
            "  + Pretending golf is fun. = I'm out of coffee, so the only thing keepig me alive right now is Pretending golf is fun..\n",
            "\n",
            "Black Card: We need to prioritize scientific research on _.\n",
            "  + The sweaty hand of a child = We need to prioritize scientific research on The sweaty hand of a child.\n",
            "  + Cheesman Park: where you can bury a bone AND uncover one. = We need to prioritize scientific research on Cheesman Park: where you can bury a bone AND uncover one..\n",
            "  + Getting locked inside a museum = We need to prioritize scientific research on Getting locked inside a museum.\n",
            "  + Asian girls with names like \"Christina\" or \"Elizabeth\". = We need to prioritize scientific research on Asian girls with names like \"Christina\" or \"Elizabeth\"..\n",
            "  + Pretending golf is fun. = We need to prioritize scientific research on Pretending golf is fun..\n"
          ]
        }
      ],
      "source": [
        "!python src/analyze_data.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlV4Sa3QpeTi"
      },
      "source": [
        "\n",
        "*   Black cards are significantly longer (mean: 54.1 chars) with more variation. White cards are shorter (mean: 29.8 chars) and more concise.\n",
        "\n",
        "*   Black cards: Common words are structural (\"like\", \"new\"). White cards: More action/descriptive words (\"getting\", \"sex\")\n",
        "\n",
        "*   Both types show a wide range of sentiment.\n",
        "*   Use separate models for black and white cards due to their different characteristics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8BsQdZywjLo",
        "outputId": "f77d3b76-ab74-4f00-b2eb-879fc38992c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch>=2.0.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from -r python-backend/requirements.txt (line 1)) (2.7.0+cu118)\n",
            "Requirement already satisfied: torchvision in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from -r python-backend/requirements.txt (line 2)) (0.22.0+cu118)\n",
            "Requirement already satisfied: torchaudio in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from -r python-backend/requirements.txt (line 3)) (2.7.0+cu118)\n",
            "Requirement already satisfied: transformers>=4.30.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from -r python-backend/requirements.txt (line 4)) (4.38.1)\n",
            "Requirement already satisfied: datasets>=2.12.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from -r python-backend/requirements.txt (line 5)) (3.6.0)\n",
            "Requirement already satisfied: accelerate in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from -r python-backend/requirements.txt (line 6)) (1.6.0)\n",
            "Requirement already satisfied: evaluate>=0.4.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from -r python-backend/requirements.txt (line 7)) (0.4.3)\n",
            "Requirement already satisfied: peft in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from -r python-backend/requirements.txt (line 8)) (0.15.2)\n",
            "Requirement already satisfied: bitsandbytes in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from -r python-backend/requirements.txt (line 9)) (0.45.5)\n",
            "Requirement already satisfied: sentencepiece in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from -r python-backend/requirements.txt (line 10)) (0.2.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from -r python-backend/requirements.txt (line 11)) (1.6.1)\n",
            "Requirement already satisfied: pandas>=1.5.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from -r python-backend/requirements.txt (line 12)) (2.2.3)\n",
            "Requirement already satisfied: tqdm in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from -r python-backend/requirements.txt (line 13)) (4.67.1)\n",
            "Requirement already satisfied: wandb in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from -r python-backend/requirements.txt (line 14)) (0.19.11)\n",
            "Requirement already satisfied: fastapi==0.110.1 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from -r python-backend/requirements.txt (line 15)) (0.110.1)\n",
            "Requirement already satisfied: uvicorn==0.30.1 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from -r python-backend/requirements.txt (line 16)) (0.30.1)\n",
            "Requirement already satisfied: pydantic==2.6.3 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from -r python-backend/requirements.txt (line 17)) (2.6.3)\n",
            "Requirement already satisfied: python-dotenv==1.0.1 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from -r python-backend/requirements.txt (line 18)) (1.0.1)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from -r python-backend/requirements.txt (line 19)) (3.10.1)\n",
            "Requirement already satisfied: seaborn>=0.12.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from -r python-backend/requirements.txt (line 20)) (0.13.2)\n",
            "Requirement already satisfied: wordcloud>=1.9.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from -r python-backend/requirements.txt (line 21)) (1.9.4)\n",
            "Requirement already satisfied: nltk>=3.8.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from -r python-backend/requirements.txt (line 22)) (3.9.1)\n",
            "Collecting textblob>=0.17.0 (from -r python-backend/requirements.txt (line 23))\n",
            "  Downloading textblob-0.19.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: networkx>=3.1 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from -r python-backend/requirements.txt (line 24)) (3.3)\n",
            "Collecting detoxify>=0.5.0 (from -r python-backend/requirements.txt (line 25))\n",
            "  Downloading detoxify-0.5.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from -r python-backend/requirements.txt (line 26)) (20.0.0)\n",
            "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from fastapi==0.110.1->-r python-backend/requirements.txt (line 15)) (0.37.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from fastapi==0.110.1->-r python-backend/requirements.txt (line 15)) (4.13.2)\n",
            "Requirement already satisfied: click>=7.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from uvicorn==0.30.1->-r python-backend/requirements.txt (line 16)) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from uvicorn==0.30.1->-r python-backend/requirements.txt (line 16)) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from pydantic==2.6.3->-r python-backend/requirements.txt (line 17)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from pydantic==2.6.3->-r python-backend/requirements.txt (line 17)) (2.16.3)\n",
            "Requirement already satisfied: filelock in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch>=2.0.0->-r python-backend/requirements.txt (line 1)) (3.13.1)\n",
            "Requirement already satisfied: setuptools in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch>=2.0.0->-r python-backend/requirements.txt (line 1)) (80.3.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch>=2.0.0->-r python-backend/requirements.txt (line 1)) (1.13.3)\n",
            "Requirement already satisfied: jinja2 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch>=2.0.0->-r python-backend/requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch>=2.0.0->-r python-backend/requirements.txt (line 1)) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch>=2.0.0->-r python-backend/requirements.txt (line 1)) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch>=2.0.0->-r python-backend/requirements.txt (line 1)) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch>=2.0.0->-r python-backend/requirements.txt (line 1)) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch>=2.0.0->-r python-backend/requirements.txt (line 1)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch>=2.0.0->-r python-backend/requirements.txt (line 1)) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch>=2.0.0->-r python-backend/requirements.txt (line 1)) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch>=2.0.0->-r python-backend/requirements.txt (line 1)) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch>=2.0.0->-r python-backend/requirements.txt (line 1)) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch>=2.0.0->-r python-backend/requirements.txt (line 1)) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch>=2.0.0->-r python-backend/requirements.txt (line 1)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch>=2.0.0->-r python-backend/requirements.txt (line 1)) (11.8.86)\n",
            "Requirement already satisfied: triton==3.3.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torch>=2.0.0->-r python-backend/requirements.txt (line 1)) (3.3.0)\n",
            "Requirement already satisfied: numpy in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torchvision->-r python-backend/requirements.txt (line 2)) (2.1.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from torchvision->-r python-backend/requirements.txt (line 2)) (11.0.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from transformers>=4.30.0->-r python-backend/requirements.txt (line 4)) (0.31.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from transformers>=4.30.0->-r python-backend/requirements.txt (line 4)) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from transformers>=4.30.0->-r python-backend/requirements.txt (line 4)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from transformers>=4.30.0->-r python-backend/requirements.txt (line 4)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from transformers>=4.30.0->-r python-backend/requirements.txt (line 4)) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from transformers>=4.30.0->-r python-backend/requirements.txt (line 4)) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from transformers>=4.30.0->-r python-backend/requirements.txt (line 4)) (0.5.3)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from datasets>=2.12.0->-r python-backend/requirements.txt (line 5)) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from datasets>=2.12.0->-r python-backend/requirements.txt (line 5)) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from datasets>=2.12.0->-r python-backend/requirements.txt (line 5)) (0.70.16)\n",
            "Requirement already satisfied: psutil in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from accelerate->-r python-backend/requirements.txt (line 6)) (7.0.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from scikit-learn>=1.0.0->-r python-backend/requirements.txt (line 11)) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from scikit-learn>=1.0.0->-r python-backend/requirements.txt (line 11)) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from scikit-learn>=1.0.0->-r python-backend/requirements.txt (line 11)) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from pandas>=1.5.0->-r python-backend/requirements.txt (line 12)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from pandas>=1.5.0->-r python-backend/requirements.txt (line 12)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from pandas>=1.5.0->-r python-backend/requirements.txt (line 12)) (2025.2)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from wandb->-r python-backend/requirements.txt (line 14)) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from wandb->-r python-backend/requirements.txt (line 14)) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from wandb->-r python-backend/requirements.txt (line 14)) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from wandb->-r python-backend/requirements.txt (line 14)) (6.30.2)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from wandb->-r python-backend/requirements.txt (line 14)) (2.27.0)\n",
            "Requirement already satisfied: setproctitle in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from wandb->-r python-backend/requirements.txt (line 14)) (1.3.6)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from matplotlib>=3.7.0->-r python-backend/requirements.txt (line 19)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from matplotlib>=3.7.0->-r python-backend/requirements.txt (line 19)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from matplotlib>=3.7.0->-r python-backend/requirements.txt (line 19)) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from matplotlib>=3.7.0->-r python-backend/requirements.txt (line 19)) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from matplotlib>=3.7.0->-r python-backend/requirements.txt (line 19)) (3.2.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from docker-pycreds>=0.4.0->wandb->-r python-backend/requirements.txt (line 14)) (1.17.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.12.0->-r python-backend/requirements.txt (line 5)) (3.11.18)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r python-backend/requirements.txt (line 14)) (4.0.12)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers>=4.30.0->-r python-backend/requirements.txt (line 4)) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from requests->transformers>=4.30.0->-r python-backend/requirements.txt (line 4)) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from requests->transformers>=4.30.0->-r python-backend/requirements.txt (line 4)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from requests->transformers>=4.30.0->-r python-backend/requirements.txt (line 4)) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from requests->transformers>=4.30.0->-r python-backend/requirements.txt (line 4)) (2025.4.26)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from starlette<0.38.0,>=0.37.2->fastapi==0.110.1->-r python-backend/requirements.txt (line 15)) (4.9.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0.0->-r python-backend/requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->-r python-backend/requirements.txt (line 1)) (3.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.12.0->-r python-backend/requirements.txt (line 5)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.12.0->-r python-backend/requirements.txt (line 5)) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.12.0->-r python-backend/requirements.txt (line 5)) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.12.0->-r python-backend/requirements.txt (line 5)) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.12.0->-r python-backend/requirements.txt (line 5)) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.12.0->-r python-backend/requirements.txt (line 5)) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.12.0->-r python-backend/requirements.txt (line 5)) (1.20.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from anyio<5,>=3.4.0->starlette<0.38.0,>=0.37.2->fastapi==0.110.1->-r python-backend/requirements.txt (line 15)) (1.3.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r python-backend/requirements.txt (line 14)) (5.0.2)\n",
            "Downloading textblob-0.19.0-py3-none-any.whl (624 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.3/624.3 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading detoxify-0.5.2-py3-none-any.whl (12 kB)\n",
            "Installing collected packages: textblob, detoxify\n",
            "Successfully installed detoxify-0.5.2 textblob-0.19.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEdeDBCrKrtH"
      },
      "source": [
        "# DETOXIFY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-kiEr9Ux3bV",
        "outputId": "ee2d0105-4ac8-4021-a51d-e53edf170952"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/cah-app\n",
            "total 305\n",
            "drwx------ 2 root root   4096 May  7 17:20 analysis\n",
            "-rw------- 1 root root  66791 May  7 17:26 CAH.ipynb\n",
            "drwx------ 5 root root   4096 May  7 15:58 data\n",
            "-rw------- 1 root root    393 Apr 17 13:20 eslint.config.mjs\n",
            "-rw------- 1 root root    480 Apr 17 13:20 .gitignore\n",
            "drwx------ 2 root root   4096 May  7 10:35 .next\n",
            "-rw------- 1 root root    133 Apr 17 13:20 next.config.ts\n",
            "-rw------- 1 root root    211 Apr 17 13:20 next-env.d.ts\n",
            "drwx------ 2 root root   4096 May  7 10:35 node_modules\n",
            "-rw------- 1 root root    620 Apr 17 14:10 package.json\n",
            "-rw------- 1 root root 209779 Apr 17 14:14 package-lock.json\n",
            "-rw------- 1 root root     81 Apr 17 13:20 postcss.config.mjs\n",
            "drwx------ 2 root root   4096 May  7 10:35 public\n",
            "drwx------ 5 root root   4096 May  7 16:09 python-backend\n",
            "-rw------- 1 root root   1450 Apr 17 13:20 README.md\n",
            "drwx------ 2 root root   4096 May  7 10:35 src\n",
            "-rw------- 1 root root    602 Apr 17 13:20 tsconfig.json\n",
            "total 18\n",
            "-rw------- 1 root root 9513 May  7 16:45  analyze_data.py\n",
            "-rw------- 1 root root 2983 May  7 17:24 'content_filter (1).py'\n",
            "-rw------- 1 root root  662 May  7 16:09  data_loading.py\n",
            "-rw------- 1 root root 3649 May  7 16:19  download_data.py\n",
            "Downloading: \"https://github.com/unitaryai/detoxify/releases/download/v0.1-alpha/toxic_original-c1212f89.ckpt\" to /root/.cache/torch/hub/checkpoints/toxic_original-c1212f89.ckpt\n",
            "100% 418M/418M [00:03<00:00, 136MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "config.json: 100% 570/570 [00:00<00:00, 2.85MB/s]\n",
            "tokenizer_config.json: 100% 48.0/48.0 [00:00<00:00, 393kB/s]\n",
            "vocab.txt: 100% 232k/232k [00:00<00:00, 1.57MB/s]\n",
            "tokenizer.json: 100% 466k/466k [00:00<00:00, 3.09MB/s]\n",
            "Current working directory: /content/drive/MyDrive/cah-app\n",
            "Looking for data in: /content/drive/MyDrive/cah-app/data/processed\n",
            "Loading training data...\n",
            "Filtering dataset for safe content...\n",
            "Processed 25000 cards...\n",
            "Processed 0 cards...\n",
            "Processed 16000 cards...\n",
            "Processed 3000 cards...\n",
            "Processed 20000 cards...\n",
            "Processed 19000 cards...\n",
            "Processed 18000 cards...\n",
            "Processed 8000 cards...\n",
            "Processed 11000 cards...\n",
            "Processed 15000 cards...\n",
            "Processed 27000 cards...\n",
            "Processed 9000 cards...\n",
            "Processed 6000 cards...\n",
            "Processed 17000 cards...\n",
            "Processed 1000 cards...\n",
            "Processed 21000 cards...\n",
            "Processed 5000 cards...\n",
            "Processed 13000 cards...\n",
            "Processed 22000 cards...\n",
            "Processed 14000 cards...\n",
            "Processed 24000 cards...\n",
            "Processed 28000 cards...\n",
            "Processed 26000 cards...\n",
            "Processed 4000 cards...\n",
            "Processed 10000 cards...\n",
            "Processed 7000 cards...\n",
            "Filtered dataset: 20139/24579 cards passed safety check\n",
            "Saving filtered dataset to: /content/drive/MyDrive/cah-app/data/processed/cah_train_safe.parquet\n",
            "\n",
            "Dataset Statistics:\n",
            "Original size: 24579\n",
            "Safe mode size: 20139\n",
            "Filtered out: 4440 cards\n",
            "total 18\n",
            "-rw------- 1 root root 9513 May  7 16:45 analyze_data.py\n",
            "-rw------- 1 root root 2983 May  7 17:26 content_filter.py\n",
            "-rw------- 1 root root  662 May  7 16:09 data_loading.py\n",
            "-rw------- 1 root root 3649 May  7 16:19 download_data.py\n"
          ]
        }
      ],
      "source": [
        "!python src/content_filter.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKJuBdlLJ0DS",
        "outputId": "64f722cc-8c33-4377-bb0a-8629e3f45795"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Loading datasets...\n",
            "\n",
            "=== Basic Statistics Comparison ===\n",
            "\n",
            "Dataset Sizes:\n",
            "Original dataset: 24579 cards\n",
            "Safe dataset: 20139 cards\n",
            "Filtered out: 4440 cards\n",
            "\n",
            "Card Type Distribution:\n",
            "Original dataset:\n",
            "card_type\n",
            "white    0.780585\n",
            "black    0.219415\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Safe dataset:\n",
            "card_type\n",
            "white    0.746661\n",
            "black    0.253339\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Length Statistics:\n",
            "\n",
            "Original dataset:\n",
            "             count       mean        std  min   25%   50%   75%    max\n",
            "card_type                                                             \n",
            "black       5393.0  54.082885  26.683258  2.0  36.0  50.0  68.0  908.0\n",
            "white      19186.0  29.787293  18.226663  2.0  16.0  26.0  39.0  249.0\n",
            "\n",
            "Safe dataset:\n",
            "             count       mean        std  min   25%   50%   75%    max\n",
            "card_type                                                             \n",
            "black       5102.0  53.766562  23.869626  2.0  36.0  50.0  68.0  168.0\n",
            "white      15037.0  28.961096  17.838700  2.0  16.0  25.0  37.0  249.0\n",
            "\n",
            "=== Word Frequency Comparison ===\n",
            "\n",
            "Black Cards - Most Common Words:\n",
            "Original dataset:\n",
            "[('like', 231), ('new', 204), ('name', 177), ('would', 167), ('get', 165), ('insert', 147), ('time', 140), ('one', 132), ('never', 128), ('trump', 125)]\n",
            "\n",
            "Safe dataset:\n",
            "[('like', 208), ('new', 200), ('name', 172), ('would', 161), ('get', 155), ('insert', 143), ('time', 128), ('one', 126), ('trump', 125), ('never', 124)]\n",
            "\n",
            "White Cards - Most Common Words:\n",
            "Original dataset:\n",
            "[('getting', 319), ('sex', 268), ('like', 232), ('one', 213), ('baby', 190), ('man', 172), ('people', 170), ('shit', 152), ('hot', 147), ('dick', 138)]\n",
            "\n",
            "Safe dataset:\n",
            "[('getting', 252), ('one', 174), ('baby', 155), ('like', 136), ('man', 128), ('people', 128), ('hot', 112), ('time', 111), ('old', 104), ('day', 103)]\n",
            "\n",
            "=== Blank Pattern Comparison ===\n",
            "\n",
            "Original dataset blank distribution:\n",
            "blank_count\n",
            "0      696\n",
            "1     4186\n",
            "2      474\n",
            "3       34\n",
            "4        1\n",
            "10       2\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Safe dataset blank distribution:\n",
            "blank_count\n",
            "0      651\n",
            "1     3956\n",
            "2      459\n",
            "3       33\n",
            "4        1\n",
            "10       2\n",
            "Name: count, dtype: int64\n",
            "\n",
            "=== Sentiment Analysis Comparison ===\n",
            "\n",
            "Original dataset sentiment statistics:\n",
            "             count      mean       std  min  25%  50%      75%  max\n",
            "card_type                                                          \n",
            "black       5393.0  0.068529  0.293596 -1.0  0.0  0.0  0.15625  1.0\n",
            "white      19186.0  0.012067  0.225861 -1.0  0.0  0.0  0.00000  1.0\n",
            "\n",
            "Safe dataset sentiment statistics:\n",
            "             count      mean       std  min  25%  50%   75%  max\n",
            "card_type                                                       \n",
            "black       5102.0  0.073505  0.290332 -1.0  0.0  0.0  0.16  1.0\n",
            "white      15037.0  0.022392  0.210604 -1.0  0.0  0.0  0.00  1.0\n",
            "\n",
            "=== Lexical Diversity Comparison ===\n",
            "\n",
            "Original dataset:\n",
            "Black cards lexical diversity: 0.122\n",
            "White cards lexical diversity: 0.165\n",
            "\n",
            "Safe dataset:\n",
            "Black cards lexical diversity: 0.125\n",
            "White cards lexical diversity: 0.195\n",
            "\n",
            "=== N-gram Pattern Comparison ===\n",
            "\n",
            "Original dataset:\n",
            "\n",
            "Top 5 2-grams in black cards:\n",
            "  of _: 436\n",
            "  with _: 264\n",
            "  for _: 238\n",
            "  i m: 225\n",
            "  about _: 220\n",
            "\n",
            "Top 5 3-grams in black cards:\n",
            "  _ and _: 101\n",
            "  what s the: 42\n",
            "  _ in the: 41\n",
            "  _ is the: 34\n",
            "  i don t: 33\n",
            "\n",
            "Top 5 2-grams in white cards:\n",
            "  with a: 366\n",
            "  in the: 361\n",
            "  of the: 281\n",
            "  in a: 208\n",
            "  on the: 175\n",
            "\n",
            "Top 5 3-grams in white cards:\n",
            "  the world s: 40\n",
            "  a sopping wet: 21\n",
            "  with a sopping: 20\n",
            "  some kind of: 20\n",
            "  out of the: 20\n",
            "\n",
            "Safe dataset:\n",
            "\n",
            "Top 5 2-grams in black cards:\n",
            "  of _: 423\n",
            "  with _: 252\n",
            "  for _: 231\n",
            "  about _: 206\n",
            "  i m: 205\n",
            "\n",
            "Top 5 3-grams in black cards:\n",
            "  _ and _: 96\n",
            "  _ in the: 40\n",
            "  what s the: 39\n",
            "  _ is the: 34\n",
            "  what is the: 31\n",
            "\n",
            "Top 5 2-grams in white cards:\n",
            "  in the: 275\n",
            "  of the: 239\n",
            "  in a: 165\n",
            "  with a: 158\n",
            "  on the: 133\n",
            "\n",
            "Top 5 3-grams in white cards:\n",
            "  the world s: 19\n",
            "  donald trump s: 16\n",
            "  to be a: 13\n",
            "  i don t: 12\n",
            "  you don t: 11\n"
          ]
        }
      ],
      "source": [
        "!python src/compare_datasets.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po26W1OnK02B"
      },
      "source": [
        "4,440 cards were filtered out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aKfpy3Zd5ln",
        "outputId": "72de7ad0-840a-475a-ca3b-51b86b2af345"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: nltk in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (3.9.1)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting absl-py (from rouge_score)\n",
            "  Downloading absl_py-2.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: numpy in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from rouge_score) (2.1.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from nltk) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
            "Collecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting tabulate>=0.8.9 (from sacrebleu)\n",
            "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting lxml (from sacrebleu)\n",
            "  Downloading lxml-5.4.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.5 kB)\n",
            "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
            "Downloading absl_py-2.2.2-py3-none-any.whl (135 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading lxml-5.4.0-cp312-cp312-manylinux_2_28_x86_64.whl (5.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m157.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24985 sha256=c3f7b27e51e0cae937f1cb21d0501ce87c1753d9c01b81f860a96e784230dfa7\n",
            "  Stored in directory: /homes/ag724/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: tabulate, portalocker, lxml, colorama, absl-py, sacrebleu, rouge_score\n",
            "Successfully installed absl-py-2.2.2 colorama-0.4.6 lxml-5.4.0 portalocker-3.1.1 rouge_score-0.1.2 sacrebleu-2.5.1 tabulate-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge_score nltk sacrebleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bFh15hAQq1x",
        "outputId": "2ccdd1fc-8e23-49ab-c45e-045aba62cc4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-05-07 19:56:32.897958: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1746647792.921742   14002 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1746647792.928820   14002 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-07 19:56:32.951829: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO:__main__:Loading data...\n",
            "INFO:__main__:\n",
            "Training BART...\n",
            "  0% 0/6798 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/data/data_collator.py:741: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
            "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n",
            "{'loss': 1.7255, 'grad_norm': 1.232354998588562, 'learning_rate': 1.9720506031185644e-05, 'epoch': 0.04}\n",
            "  1% 100/6798 [00:36<41:05,  2.72it/s]\n",
            "  0% 0/252 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/252 [00:04<08:35,  2.06s/it]\u001b[A\n",
            "  1% 3/252 [00:08<13:07,  3.16s/it]\u001b[A\n",
            "  2% 4/252 [00:13<14:47,  3.58s/it]\u001b[A\n",
            "  2% 5/252 [00:17<16:01,  3.89s/it]\u001b[A\n",
            "  2% 6/252 [00:22<17:12,  4.20s/it]\u001b[A\n",
            "  3% 7/252 [00:26<17:04,  4.18s/it]\u001b[A\n",
            "  3% 8/252 [00:30<16:57,  4.17s/it]\u001b[A\n",
            "  4% 9/252 [00:35<17:37,  4.35s/it]\u001b[A\n",
            "  4% 10/252 [00:40<17:48,  4.41s/it]\u001b[A\n",
            "  4% 11/252 [00:44<17:20,  4.32s/it]\u001b[A\n",
            "  5% 12/252 [00:48<17:56,  4.49s/it]\u001b[A\n",
            "  5% 13/252 [00:53<17:24,  4.37s/it]\u001b[A\n",
            "  6% 14/252 [00:57<17:02,  4.30s/it]\u001b[A\n",
            "  6% 15/252 [01:01<17:29,  4.43s/it]\u001b[A\n",
            "  6% 16/252 [01:06<17:25,  4.43s/it]\u001b[A\n",
            "  7% 17/252 [01:10<16:57,  4.33s/it]\u001b[A\n",
            "  7% 18/252 [01:15<17:18,  4.44s/it]\u001b[A\n",
            "  8% 19/252 [01:19<16:47,  4.32s/it]\u001b[A\n",
            "  8% 20/252 [01:23<16:29,  4.26s/it]\u001b[A\n",
            "  8% 21/252 [01:28<17:27,  4.53s/it]\u001b[A\n",
            "  9% 22/252 [01:32<16:54,  4.41s/it]\u001b[A\n",
            "  9% 23/252 [01:36<16:29,  4.32s/it]\u001b[A\n",
            " 10% 24/252 [01:41<16:58,  4.47s/it]\u001b[A\n",
            " 10% 25/252 [01:45<16:43,  4.42s/it]\u001b[A\n",
            " 10% 26/252 [01:50<16:53,  4.49s/it]\u001b[A\n",
            " 11% 27/252 [01:55<17:42,  4.72s/it]\u001b[A\n",
            " 11% 28/252 [01:59<17:00,  4.56s/it]\u001b[A\n",
            " 12% 29/252 [02:04<16:28,  4.43s/it]\u001b[A\n",
            " 12% 30/252 [02:09<17:00,  4.60s/it]\u001b[A\n",
            " 12% 31/252 [02:13<16:26,  4.46s/it]\u001b[A\n",
            " 13% 32/252 [02:17<16:22,  4.47s/it]\u001b[A\n",
            " 13% 33/252 [02:22<16:52,  4.62s/it]\u001b[A\n",
            " 13% 34/252 [02:26<16:19,  4.49s/it]\u001b[A\n",
            " 14% 35/252 [02:31<15:53,  4.39s/it]\u001b[A\n",
            " 14% 36/252 [02:36<16:27,  4.57s/it]\u001b[A\n",
            " 15% 37/252 [02:40<16:18,  4.55s/it]\u001b[A\n",
            " 15% 38/252 [02:44<15:52,  4.45s/it]\u001b[A\n",
            " 15% 39/252 [02:49<16:12,  4.56s/it]\u001b[A\n",
            " 16% 40/252 [02:53<15:41,  4.44s/it]\u001b[A\n",
            " 16% 41/252 [02:57<15:26,  4.39s/it]\u001b[A\n",
            " 17% 42/252 [03:03<16:11,  4.63s/it]\u001b[A\n",
            " 17% 43/252 [03:07<15:37,  4.48s/it]\u001b[A\n",
            " 17% 44/252 [03:11<15:18,  4.42s/it]\u001b[A\n",
            " 18% 45/252 [03:16<15:32,  4.50s/it]\u001b[A\n",
            " 18% 46/252 [03:20<15:03,  4.39s/it]\u001b[A\n",
            " 19% 47/252 [03:25<15:15,  4.47s/it]\u001b[A\n",
            " 19% 48/252 [03:29<15:22,  4.52s/it]\u001b[A\n",
            " 19% 49/252 [03:33<14:53,  4.40s/it]\u001b[A\n",
            " 20% 50/252 [03:38<14:38,  4.35s/it]\u001b[A\n",
            " 20% 51/252 [03:42<14:50,  4.43s/it]\u001b[A\n",
            " 21% 52/252 [03:47<14:47,  4.44s/it]\u001b[A\n",
            " 21% 53/252 [03:51<14:32,  4.38s/it]\u001b[A\n",
            " 21% 54/252 [03:55<14:38,  4.44s/it]\u001b[A\n",
            " 22% 55/252 [04:00<14:17,  4.35s/it]\u001b[A\n",
            " 22% 56/252 [04:04<14:04,  4.31s/it]\u001b[A\n",
            " 23% 57/252 [04:09<14:32,  4.48s/it]\u001b[A\n",
            " 23% 58/252 [04:13<13:57,  4.32s/it]\u001b[A\n",
            " 23% 59/252 [04:17<13:38,  4.24s/it]\u001b[A\n",
            " 24% 60/252 [04:21<13:49,  4.32s/it]\u001b[A\n",
            " 24% 61/252 [04:25<13:24,  4.21s/it]\u001b[A\n",
            " 25% 62/252 [04:30<13:29,  4.26s/it]\u001b[A\n",
            " 25% 63/252 [04:34<13:40,  4.34s/it]\u001b[A\n",
            " 25% 64/252 [04:38<13:19,  4.25s/it]\u001b[A\n",
            " 26% 65/252 [04:42<12:58,  4.16s/it]\u001b[A\n",
            " 26% 66/252 [04:47<13:21,  4.31s/it]\u001b[A\n",
            " 27% 67/252 [04:51<13:17,  4.31s/it]\u001b[A\n",
            " 27% 68/252 [04:55<12:52,  4.20s/it]\u001b[A\n",
            " 27% 69/252 [05:00<13:18,  4.36s/it]\u001b[A\n",
            " 28% 70/252 [05:04<12:50,  4.23s/it]\u001b[A\n",
            " 28% 71/252 [05:08<12:29,  4.14s/it]\u001b[A\n",
            " 29% 72/252 [05:12<12:56,  4.31s/it]\u001b[A\n",
            " 29% 73/252 [05:17<12:49,  4.30s/it]\u001b[A\n",
            " 29% 74/252 [05:20<12:27,  4.20s/it]\u001b[A\n",
            " 30% 75/252 [05:25<12:47,  4.33s/it]\u001b[A\n",
            " 30% 76/252 [05:29<12:26,  4.24s/it]\u001b[A\n",
            " 31% 77/252 [05:33<12:08,  4.17s/it]\u001b[A\n",
            " 31% 78/252 [05:38<12:51,  4.43s/it]\u001b[A\n",
            " 31% 79/252 [05:43<12:57,  4.49s/it]\u001b[A\n",
            " 32% 80/252 [05:47<12:25,  4.33s/it]\u001b[A\n",
            " 32% 81/252 [05:52<12:39,  4.44s/it]\u001b[A\n",
            " 33% 82/252 [05:55<12:11,  4.30s/it]\u001b[A\n",
            " 33% 83/252 [06:00<12:08,  4.31s/it]\u001b[A\n",
            " 33% 84/252 [06:05<12:24,  4.43s/it]\u001b[A\n",
            " 34% 85/252 [06:08<11:56,  4.29s/it]\u001b[A\n",
            " 34% 86/252 [06:12<11:34,  4.19s/it]\u001b[A\n",
            " 35% 87/252 [06:17<11:53,  4.32s/it]\u001b[A\n",
            " 35% 88/252 [06:21<11:47,  4.31s/it]\u001b[A\n",
            " 35% 89/252 [06:25<11:27,  4.22s/it]\u001b[A\n",
            " 36% 90/252 [06:30<11:46,  4.36s/it]\u001b[A\n",
            " 36% 91/252 [06:34<11:23,  4.25s/it]\u001b[A\n",
            " 37% 92/252 [06:38<11:06,  4.17s/it]\u001b[A\n",
            " 37% 93/252 [06:43<11:36,  4.38s/it]\u001b[A\n",
            " 37% 94/252 [06:47<11:12,  4.26s/it]\u001b[A\n",
            " 38% 95/252 [06:51<10:55,  4.17s/it]\u001b[A\n",
            " 38% 96/252 [06:55<11:05,  4.26s/it]\u001b[A\n",
            " 38% 97/252 [06:59<10:49,  4.19s/it]\u001b[A\n",
            " 39% 98/252 [07:03<10:36,  4.13s/it]\u001b[A\n",
            " 39% 99/252 [07:08<10:57,  4.30s/it]\u001b[A\n",
            " 40% 100/252 [07:12<10:43,  4.24s/it]\u001b[A\n",
            " 40% 101/252 [07:16<10:27,  4.16s/it]\u001b[A\n",
            " 40% 102/252 [07:21<10:37,  4.25s/it]\u001b[A\n",
            " 41% 103/252 [07:25<10:39,  4.29s/it]\u001b[A\n",
            " 41% 104/252 [07:29<10:42,  4.34s/it]\u001b[A\n",
            " 42% 105/252 [07:34<10:45,  4.39s/it]\u001b[A\n",
            " 42% 106/252 [07:38<10:39,  4.38s/it]\u001b[A\n",
            " 42% 107/252 [07:42<10:21,  4.29s/it]\u001b[A\n",
            " 43% 108/252 [07:47<10:40,  4.45s/it]\u001b[A\n",
            " 43% 109/252 [07:52<10:32,  4.42s/it]\u001b[A\n",
            " 44% 110/252 [07:56<10:14,  4.33s/it]\u001b[A\n",
            " 44% 111/252 [08:00<10:18,  4.39s/it]\u001b[A\n",
            " 44% 112/252 [08:05<10:14,  4.39s/it]\u001b[A\n",
            " 45% 113/252 [08:09<09:56,  4.29s/it]\u001b[A\n",
            " 45% 114/252 [08:13<10:06,  4.39s/it]\u001b[A\n",
            " 46% 115/252 [08:17<09:56,  4.35s/it]\u001b[A\n",
            " 46% 116/252 [08:21<09:36,  4.24s/it]\u001b[A\n",
            " 46% 117/252 [08:26<09:30,  4.23s/it]\u001b[A\n",
            " 47% 118/252 [08:30<09:34,  4.29s/it]\u001b[A\n",
            " 47% 119/252 [08:34<09:29,  4.28s/it]\u001b[A\n",
            " 48% 120/252 [08:38<09:18,  4.23s/it]\u001b[A\n",
            " 48% 121/252 [08:43<09:22,  4.29s/it]\u001b[A\n",
            " 48% 122/252 [08:47<09:04,  4.19s/it]\u001b[A\n",
            " 49% 123/252 [08:51<08:54,  4.15s/it]\u001b[A\n",
            " 49% 124/252 [08:56<09:19,  4.37s/it]\u001b[A\n",
            " 50% 125/252 [09:00<09:01,  4.26s/it]\u001b[A\n",
            " 50% 126/252 [09:04<08:47,  4.18s/it]\u001b[A\n",
            " 50% 127/252 [09:08<08:58,  4.31s/it]\u001b[A\n",
            " 51% 128/252 [09:12<08:40,  4.20s/it]\u001b[A\n",
            " 51% 129/252 [09:17<08:39,  4.22s/it]\u001b[A\n",
            " 52% 130/252 [09:21<08:48,  4.33s/it]\u001b[A\n",
            " 52% 131/252 [09:25<08:30,  4.22s/it]\u001b[A\n",
            " 52% 132/252 [09:29<08:17,  4.15s/it]\u001b[A\n",
            " 53% 133/252 [09:34<08:37,  4.35s/it]\u001b[A\n",
            " 53% 134/252 [09:39<08:41,  4.42s/it]\u001b[A\n",
            " 54% 135/252 [09:43<08:33,  4.39s/it]\u001b[A\n",
            " 54% 136/252 [09:47<08:34,  4.44s/it]\u001b[A\n",
            " 54% 137/252 [09:51<08:14,  4.30s/it]\u001b[A\n",
            " 55% 138/252 [09:55<07:58,  4.19s/it]\u001b[A\n",
            " 55% 139/252 [10:00<08:13,  4.37s/it]\u001b[A\n",
            " 56% 140/252 [10:04<07:55,  4.25s/it]\u001b[A\n",
            " 56% 141/252 [10:08<07:53,  4.26s/it]\u001b[A\n",
            " 56% 142/252 [10:13<08:06,  4.42s/it]\u001b[A\n",
            " 57% 143/252 [10:17<07:46,  4.28s/it]\u001b[A\n",
            " 57% 144/252 [10:21<07:31,  4.18s/it]\u001b[A\n",
            " 58% 145/252 [10:26<07:43,  4.33s/it]\u001b[A\n",
            " 58% 146/252 [10:30<07:38,  4.33s/it]\u001b[A\n",
            " 58% 147/252 [10:34<07:22,  4.22s/it]\u001b[A\n",
            " 59% 148/252 [10:39<07:33,  4.36s/it]\u001b[A\n",
            " 59% 149/252 [10:43<07:17,  4.24s/it]\u001b[A\n",
            " 60% 150/252 [10:47<07:03,  4.15s/it]\u001b[A\n",
            " 60% 151/252 [10:52<07:23,  4.39s/it]\u001b[A\n",
            " 60% 152/252 [10:56<07:05,  4.26s/it]\u001b[A\n",
            " 61% 153/252 [11:00<06:54,  4.18s/it]\u001b[A\n",
            " 61% 154/252 [11:04<07:02,  4.31s/it]\u001b[A\n",
            " 62% 155/252 [11:08<06:48,  4.21s/it]\u001b[A\n",
            " 62% 156/252 [11:12<06:47,  4.24s/it]\u001b[A\n",
            " 62% 157/252 [11:17<06:52,  4.34s/it]\u001b[A\n",
            " 63% 158/252 [11:21<06:37,  4.22s/it]\u001b[A\n",
            " 63% 159/252 [11:25<06:25,  4.15s/it]\u001b[A\n",
            " 63% 160/252 [11:29<06:28,  4.22s/it]\u001b[A\n",
            " 64% 161/252 [11:34<06:30,  4.29s/it]\u001b[A\n",
            " 64% 162/252 [11:38<06:17,  4.20s/it]\u001b[A\n",
            " 65% 163/252 [11:42<06:16,  4.23s/it]\u001b[A\n",
            " 65% 164/252 [11:46<06:12,  4.23s/it]\u001b[A\n",
            " 65% 165/252 [11:50<06:01,  4.15s/it]\u001b[A\n",
            " 66% 166/252 [11:55<06:08,  4.29s/it]\u001b[A\n",
            " 66% 167/252 [11:59<06:04,  4.29s/it]\u001b[A\n",
            " 67% 168/252 [12:03<05:53,  4.20s/it]\u001b[A\n",
            " 67% 169/252 [12:07<05:47,  4.19s/it]\u001b[A\n",
            " 67% 170/252 [12:12<05:49,  4.26s/it]\u001b[A\n",
            " 68% 171/252 [12:16<05:45,  4.27s/it]\u001b[A\n",
            " 68% 172/252 [12:20<05:41,  4.27s/it]\u001b[A\n",
            " 69% 173/252 [12:25<05:45,  4.37s/it]\u001b[A\n",
            " 69% 174/252 [12:29<05:34,  4.29s/it]\u001b[A\n",
            " 69% 175/252 [12:33<05:30,  4.29s/it]\u001b[A\n",
            " 70% 176/252 [12:38<05:41,  4.50s/it]\u001b[A\n",
            " 70% 177/252 [12:42<05:28,  4.38s/it]\u001b[A\n",
            " 71% 178/252 [12:47<05:22,  4.36s/it]\u001b[A\n",
            " 71% 179/252 [12:51<05:22,  4.42s/it]\u001b[A\n",
            " 71% 180/252 [12:55<05:11,  4.32s/it]\u001b[A\n",
            " 72% 181/252 [13:00<05:02,  4.26s/it]\u001b[A\n",
            " 72% 182/252 [13:04<05:11,  4.45s/it]\u001b[A\n",
            " 73% 183/252 [13:08<04:57,  4.31s/it]\u001b[A\n",
            " 73% 184/252 [13:12<04:48,  4.24s/it]\u001b[A\n",
            " 73% 185/252 [13:17<04:51,  4.35s/it]\u001b[A\n",
            " 74% 186/252 [13:21<04:41,  4.26s/it]\u001b[A\n",
            " 74% 187/252 [13:26<04:43,  4.37s/it]\u001b[A\n",
            " 75% 188/252 [13:31<05:03,  4.74s/it]\u001b[A\n",
            " 75% 189/252 [13:35<04:43,  4.51s/it]\u001b[A\n",
            " 75% 190/252 [13:39<04:29,  4.35s/it]\u001b[A\n",
            " 76% 191/252 [13:44<04:29,  4.42s/it]\u001b[A\n",
            " 76% 192/252 [13:48<04:17,  4.29s/it]\u001b[A\n",
            " 77% 193/252 [13:52<04:12,  4.28s/it]\u001b[A\n",
            " 77% 194/252 [13:57<04:12,  4.35s/it]\u001b[A\n",
            " 77% 195/252 [14:01<04:03,  4.27s/it]\u001b[A\n",
            " 78% 196/252 [14:05<03:54,  4.18s/it]\u001b[A\n",
            " 78% 197/252 [14:09<03:54,  4.26s/it]\u001b[A\n",
            " 79% 198/252 [14:14<03:53,  4.33s/it]\u001b[A\n",
            " 79% 199/252 [14:18<03:43,  4.22s/it]\u001b[A\n",
            " 79% 200/252 [14:22<03:40,  4.24s/it]\u001b[A\n",
            " 80% 201/252 [14:26<03:36,  4.24s/it]\u001b[A\n",
            " 80% 202/252 [14:30<03:33,  4.26s/it]\u001b[A\n",
            " 81% 203/252 [14:35<03:28,  4.26s/it]\u001b[A\n",
            " 81% 204/252 [14:39<03:25,  4.27s/it]\u001b[A\n",
            " 81% 205/252 [14:43<03:16,  4.18s/it]\u001b[A\n",
            " 82% 206/252 [14:47<03:12,  4.18s/it]\u001b[A\n",
            " 82% 207/252 [14:52<03:11,  4.26s/it]\u001b[A\n",
            " 83% 208/252 [14:56<03:08,  4.29s/it]\u001b[A\n",
            " 83% 209/252 [15:00<03:04,  4.29s/it]\u001b[A\n",
            " 83% 210/252 [15:05<03:03,  4.37s/it]\u001b[A\n",
            " 84% 211/252 [15:09<02:55,  4.28s/it]\u001b[A\n",
            " 84% 212/252 [15:13<02:49,  4.24s/it]\u001b[A\n",
            " 85% 213/252 [15:18<02:53,  4.46s/it]\u001b[A\n",
            " 85% 214/252 [15:22<02:45,  4.34s/it]\u001b[A\n",
            " 85% 215/252 [15:26<02:39,  4.30s/it]\u001b[A\n",
            " 86% 216/252 [15:31<02:38,  4.41s/it]\u001b[A\n",
            " 86% 217/252 [15:35<02:30,  4.29s/it]\u001b[A\n",
            " 87% 218/252 [15:39<02:28,  4.36s/it]\u001b[A\n",
            " 87% 219/252 [15:44<02:25,  4.40s/it]\u001b[A\n",
            " 87% 220/252 [15:48<02:16,  4.27s/it]\u001b[A\n",
            " 88% 221/252 [15:52<02:10,  4.20s/it]\u001b[A\n",
            " 88% 222/252 [15:57<02:09,  4.33s/it]\u001b[A\n",
            " 88% 223/252 [16:01<02:05,  4.34s/it]\u001b[A\n",
            " 89% 224/252 [16:05<01:58,  4.24s/it]\u001b[A\n",
            " 89% 225/252 [16:10<01:57,  4.36s/it]\u001b[A\n",
            " 90% 226/252 [16:14<01:50,  4.23s/it]\u001b[A\n",
            " 90% 227/252 [16:17<01:43,  4.15s/it]\u001b[A\n",
            " 90% 228/252 [16:23<01:45,  4.42s/it]\u001b[A\n",
            " 91% 229/252 [16:26<01:38,  4.28s/it]\u001b[A\n",
            " 91% 230/252 [16:30<01:32,  4.19s/it]\u001b[A\n",
            " 92% 231/252 [16:35<01:31,  4.34s/it]\u001b[A\n",
            " 92% 232/252 [16:39<01:24,  4.23s/it]\u001b[A\n",
            " 92% 233/252 [16:43<01:18,  4.15s/it]\u001b[A\n",
            " 93% 234/252 [16:48<01:19,  4.40s/it]\u001b[A\n",
            " 93% 235/252 [16:52<01:12,  4.27s/it]\u001b[A\n",
            " 94% 236/252 [16:56<01:06,  4.18s/it]\u001b[A\n",
            " 94% 237/252 [17:01<01:05,  4.34s/it]\u001b[A\n",
            " 94% 238/252 [17:05<00:59,  4.22s/it]\u001b[A\n",
            " 95% 239/252 [17:09<00:55,  4.27s/it]\u001b[A\n",
            " 95% 240/252 [17:14<00:53,  4.42s/it]\u001b[A\n",
            " 96% 241/252 [17:19<00:49,  4.52s/it]\u001b[A\n",
            " 96% 242/252 [17:23<00:44,  4.40s/it]\u001b[A\n",
            " 96% 243/252 [17:28<00:41,  4.56s/it]\u001b[A\n",
            " 97% 244/252 [17:32<00:36,  4.54s/it]\u001b[A\n",
            " 97% 245/252 [17:36<00:30,  4.41s/it]\u001b[A\n",
            " 98% 246/252 [17:41<00:27,  4.56s/it]\u001b[A\n",
            " 98% 247/252 [17:45<00:22,  4.43s/it]\u001b[A\n",
            " 98% 248/252 [17:49<00:17,  4.33s/it]\u001b[A\n",
            " 99% 249/252 [17:55<00:13,  4.58s/it]\u001b[A\n",
            " 99% 250/252 [17:58<00:08,  4.39s/it]\u001b[A\n",
            "100% 251/252 [18:02<00:04,  4.27s/it]\u001b[A\n",
            "100% 252/252 [18:07<00:00,  4.30s/it]\u001b[A\n",
            "\n",
            "Downloading builder script: 100% 6.27k/6.27k [00:00<00:00, 17.9MB/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/cah-app/python-backend/src/model_comparison.py\", line 221, in <module>\n",
            "  File \"/content/drive/MyDrive/cah-app/python-backend/src/model_comparison.py\", line 203, in main\n",
            "    main() \n",
            "        ^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 2245, in train\n",
            "    return inner_training_loop(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 2627, in _inner_training_loop\n",
            "    self._maybe_log_save_evaluate(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 3096, in _maybe_log_save_evaluate\n",
            "    metrics = self._evaluate(trial, ignore_keys_for_eval)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 3045, in _evaluate\n",
            "    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer_seq2seq.py\", line 197, in evaluate\n",
            "    return super().evaluate(eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 4154, in evaluate\n",
            "    output = eval_loop(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 4443, in evaluation_loop\n",
            "    metrics = self.compute_metrics(\n",
            "              ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/cah-app/python-backend/src/model_comparison.py\", line 113, in compute_metrics\n",
            "    # Training arguments\n",
            "             ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/evaluate/loading.py\", line 748, in load\n",
            "    evaluation_module = evaluation_module_factory(\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/evaluate/loading.py\", line 680, in evaluation_module_factory\n",
            "    raise e1 from None\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/evaluate/loading.py\", line 639, in evaluation_module_factory\n",
            "    ).get_module()\n",
            "      ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/evaluate/loading.py\", line 489, in get_module\n",
            "    local_imports = _download_additional_modules(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/evaluate/loading.py\", line 265, in _download_additional_modules\n",
            "    raise ImportError(\n",
            "ImportError: To be able to use evaluate-metric/rouge, you need to install the following dependencies['rouge_score'] using 'pip install rouge_score' for instance'\n",
            "  1%|▏         | 100/6798 [18:51<21:02:50, 11.31s/it]\n",
            "\n",
            "                                     \u001b[A"
          ]
        }
      ],
      "source": [
        "!python src/model_comparison.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoztloGxfibI",
        "outputId": "a371d44b-a187-4723-f360-249fd2a970f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-05-08 10:51:14.445336: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1746701474.466656    3116 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1746701474.473202    3116 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-08 10:51:14.494336: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO:__main__:Loading data...\n",
            "INFO:__main__:\n",
            "Training BART...\n",
            "Map: 100% 18125/18125 [00:03<00:00, 5610.81 examples/s]\n",
            "Map: 100% 2014/2014 [00:00<00:00, 3895.14 examples/s]\n",
            "INFO:__main__:No checkpoint found. Starting from pretrained model.\n",
            "{'loss': 1.7748, 'grad_norm': 1.3353055715560913, 'learning_rate': 1.9720506031185644e-05, 'epoch': 0.04}\n",
            "  1% 100/6798 [00:36<40:44,  2.74it/s]\n",
            "  0% 0/252 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/252 [00:03<08:12,  1.97s/it]\u001b[A\n",
            "  1% 3/252 [00:07<11:44,  2.83s/it]\u001b[A\n",
            "  2% 4/252 [00:12<14:22,  3.48s/it]\u001b[A\n",
            "  2% 5/252 [00:16<15:25,  3.75s/it]\u001b[A\n",
            "  2% 6/252 [00:20<15:42,  3.83s/it]\u001b[A\n",
            "  3% 7/252 [00:25<16:41,  4.09s/it]\u001b[A\n",
            "  3% 8/252 [00:29<16:26,  4.04s/it]\u001b[A\n",
            "  4% 9/252 [00:33<16:18,  4.03s/it]\u001b[A\n",
            "  4% 10/252 [00:38<17:31,  4.35s/it]\u001b[A\n",
            "  4% 11/252 [00:42<16:58,  4.23s/it]\u001b[A\n",
            "  5% 12/252 [00:46<16:36,  4.15s/it]\u001b[A\n",
            "  5% 13/252 [00:51<17:17,  4.34s/it]\u001b[A\n",
            "  6% 14/252 [00:55<16:48,  4.24s/it]\u001b[A\n",
            "  6% 15/252 [00:59<16:48,  4.26s/it]\u001b[A\n",
            "  6% 16/252 [01:04<17:18,  4.40s/it]\u001b[A\n",
            "  7% 17/252 [01:08<16:44,  4.27s/it]\u001b[A\n",
            "  7% 18/252 [01:12<16:19,  4.18s/it]\u001b[A\n",
            "  8% 19/252 [01:16<16:53,  4.35s/it]\u001b[A\n",
            "  8% 20/252 [01:21<16:47,  4.34s/it]\u001b[A\n",
            "  8% 21/252 [01:25<16:21,  4.25s/it]\u001b[A\n",
            "  9% 22/252 [01:29<16:50,  4.40s/it]\u001b[A\n",
            "  9% 23/252 [01:33<16:19,  4.28s/it]\u001b[A\n",
            " 10% 24/252 [01:37<15:57,  4.20s/it]\u001b[A\n",
            " 10% 25/252 [01:42<16:46,  4.44s/it]\u001b[A\n",
            " 10% 26/252 [01:46<16:13,  4.31s/it]\u001b[A\n",
            " 11% 27/252 [01:50<15:48,  4.22s/it]\u001b[A\n",
            " 11% 28/252 [01:55<16:15,  4.35s/it]\u001b[A\n",
            " 12% 29/252 [01:59<15:46,  4.24s/it]\u001b[A\n",
            " 12% 30/252 [02:03<15:47,  4.27s/it]\u001b[A\n",
            " 12% 31/252 [02:08<16:11,  4.40s/it]\u001b[A\n",
            " 13% 32/252 [02:12<15:40,  4.27s/it]\u001b[A\n",
            " 13% 33/252 [02:16<15:17,  4.19s/it]\u001b[A\n",
            " 13% 34/252 [02:21<15:42,  4.32s/it]\u001b[A\n",
            " 14% 35/252 [02:25<15:37,  4.32s/it]\u001b[A\n",
            " 14% 36/252 [02:29<15:10,  4.21s/it]\u001b[A\n",
            " 15% 37/252 [02:34<15:32,  4.34s/it]\u001b[A\n",
            " 15% 38/252 [02:38<15:05,  4.23s/it]\u001b[A\n",
            " 15% 39/252 [02:42<14:45,  4.16s/it]\u001b[A\n",
            " 16% 40/252 [02:46<15:23,  4.35s/it]\u001b[A\n",
            " 16% 41/252 [02:50<14:58,  4.26s/it]\u001b[A\n",
            " 17% 42/252 [02:55<14:40,  4.19s/it]\u001b[A\n",
            " 17% 43/252 [02:59<14:48,  4.25s/it]\u001b[A\n",
            " 17% 44/252 [03:03<14:41,  4.24s/it]\u001b[A\n",
            " 18% 45/252 [03:07<14:22,  4.17s/it]\u001b[A\n",
            " 18% 46/252 [03:12<14:47,  4.31s/it]\u001b[A\n",
            " 19% 47/252 [03:16<14:39,  4.29s/it]\u001b[A\n",
            " 19% 48/252 [03:20<14:17,  4.20s/it]\u001b[A\n",
            " 19% 49/252 [03:24<14:19,  4.23s/it]\u001b[A\n",
            " 20% 50/252 [03:29<14:39,  4.35s/it]\u001b[A\n",
            " 20% 51/252 [03:33<14:12,  4.24s/it]\u001b[A\n",
            " 21% 52/252 [03:37<14:07,  4.24s/it]\u001b[A\n",
            " 21% 53/252 [03:42<14:10,  4.27s/it]\u001b[A\n",
            " 21% 54/252 [03:46<13:50,  4.20s/it]\u001b[A\n",
            " 22% 55/252 [03:50<14:12,  4.33s/it]\u001b[A\n",
            " 22% 56/252 [03:55<14:12,  4.35s/it]\u001b[A\n",
            " 23% 57/252 [03:59<13:47,  4.24s/it]\u001b[A\n",
            " 23% 58/252 [04:03<13:37,  4.21s/it]\u001b[A\n",
            " 23% 59/252 [04:07<13:54,  4.32s/it]\u001b[A\n",
            " 24% 60/252 [04:12<13:50,  4.32s/it]\u001b[A\n",
            " 24% 61/252 [04:16<13:35,  4.27s/it]\u001b[A\n",
            " 25% 62/252 [04:20<13:46,  4.35s/it]\u001b[A\n",
            " 25% 63/252 [04:24<13:24,  4.26s/it]\u001b[A\n",
            " 25% 64/252 [04:28<13:11,  4.21s/it]\u001b[A\n",
            " 26% 65/252 [04:33<13:31,  4.34s/it]\u001b[A\n",
            " 26% 66/252 [04:37<13:29,  4.35s/it]\u001b[A\n",
            " 27% 67/252 [04:42<13:14,  4.29s/it]\u001b[A\n",
            " 27% 68/252 [04:46<13:28,  4.39s/it]\u001b[A\n",
            " 27% 69/252 [04:50<13:06,  4.30s/it]\u001b[A\n",
            " 28% 70/252 [04:54<12:52,  4.25s/it]\u001b[A\n",
            " 28% 71/252 [04:59<13:27,  4.46s/it]\u001b[A\n",
            " 29% 72/252 [05:03<13:00,  4.34s/it]\u001b[A\n",
            " 29% 73/252 [05:08<12:46,  4.28s/it]\u001b[A\n",
            " 29% 74/252 [05:12<12:59,  4.38s/it]\u001b[A\n",
            " 30% 75/252 [05:16<12:36,  4.28s/it]\u001b[A\n",
            " 30% 76/252 [05:21<12:43,  4.34s/it]\u001b[A\n",
            " 31% 77/252 [05:25<12:51,  4.41s/it]\u001b[A\n",
            " 31% 78/252 [05:29<12:26,  4.29s/it]\u001b[A\n",
            " 31% 79/252 [05:33<12:10,  4.22s/it]\u001b[A\n",
            " 32% 80/252 [05:38<12:29,  4.36s/it]\u001b[A\n",
            " 32% 81/252 [05:42<12:07,  4.25s/it]\u001b[A\n",
            " 33% 82/252 [05:46<12:08,  4.28s/it]\u001b[A\n",
            " 33% 83/252 [05:51<12:22,  4.39s/it]\u001b[A\n",
            " 33% 84/252 [05:55<11:58,  4.28s/it]\u001b[A\n",
            " 34% 85/252 [05:59<11:39,  4.19s/it]\u001b[A\n",
            " 34% 86/252 [06:04<12:03,  4.36s/it]\u001b[A\n",
            " 35% 87/252 [06:08<11:56,  4.34s/it]\u001b[A\n",
            " 35% 88/252 [06:12<11:36,  4.25s/it]\u001b[A\n",
            " 35% 89/252 [06:17<11:54,  4.38s/it]\u001b[A\n",
            " 36% 90/252 [06:21<11:31,  4.27s/it]\u001b[A\n",
            " 36% 91/252 [06:25<11:14,  4.19s/it]\u001b[A\n",
            " 37% 92/252 [06:30<11:50,  4.44s/it]\u001b[A\n",
            " 37% 93/252 [06:34<11:23,  4.30s/it]\u001b[A\n",
            " 37% 94/252 [06:38<11:04,  4.21s/it]\u001b[A\n",
            " 38% 95/252 [06:43<11:25,  4.36s/it]\u001b[A\n",
            " 38% 96/252 [06:47<11:01,  4.24s/it]\u001b[A\n",
            " 38% 97/252 [06:51<10:59,  4.26s/it]\u001b[A\n",
            " 39% 98/252 [06:56<11:17,  4.40s/it]\u001b[A\n",
            " 39% 99/252 [07:00<10:53,  4.27s/it]\u001b[A\n",
            " 40% 100/252 [07:03<10:35,  4.18s/it]\u001b[A\n",
            " 40% 101/252 [07:08<10:53,  4.33s/it]\u001b[A\n",
            " 40% 102/252 [07:12<10:48,  4.32s/it]\u001b[A\n",
            " 41% 103/252 [07:16<10:28,  4.22s/it]\u001b[A\n",
            " 41% 104/252 [07:21<10:45,  4.36s/it]\u001b[A\n",
            " 42% 105/252 [07:25<10:24,  4.25s/it]\u001b[A\n",
            " 42% 106/252 [07:29<10:08,  4.17s/it]\u001b[A\n",
            " 42% 107/252 [07:34<10:26,  4.32s/it]\u001b[A\n",
            " 43% 108/252 [07:38<10:24,  4.34s/it]\u001b[A\n",
            " 43% 109/252 [07:42<10:05,  4.23s/it]\u001b[A\n",
            " 44% 110/252 [07:47<10:20,  4.37s/it]\u001b[A\n",
            " 44% 111/252 [07:51<10:02,  4.28s/it]\u001b[A\n",
            " 44% 112/252 [07:55<09:46,  4.19s/it]\u001b[A\n",
            " 45% 113/252 [08:00<10:14,  4.42s/it]\u001b[A\n",
            " 45% 114/252 [08:04<09:51,  4.28s/it]\u001b[A\n",
            " 46% 115/252 [08:08<09:36,  4.21s/it]\u001b[A\n",
            " 46% 116/252 [08:12<09:47,  4.32s/it]\u001b[A\n",
            " 46% 117/252 [08:16<09:28,  4.21s/it]\u001b[A\n",
            " 47% 118/252 [08:21<09:27,  4.23s/it]\u001b[A\n",
            " 47% 119/252 [08:25<09:28,  4.28s/it]\u001b[A\n",
            " 48% 120/252 [08:29<09:17,  4.23s/it]\u001b[A\n",
            " 48% 121/252 [08:33<09:04,  4.16s/it]\u001b[A\n",
            " 48% 122/252 [08:37<09:04,  4.19s/it]\u001b[A\n",
            " 49% 123/252 [08:42<09:16,  4.31s/it]\u001b[A\n",
            " 49% 124/252 [08:46<08:58,  4.21s/it]\u001b[A\n",
            " 50% 125/252 [08:50<08:56,  4.22s/it]\u001b[A\n",
            " 50% 126/252 [08:55<08:57,  4.26s/it]\u001b[A\n",
            " 50% 127/252 [08:59<08:42,  4.18s/it]\u001b[A\n",
            " 51% 128/252 [09:03<08:36,  4.16s/it]\u001b[A\n",
            " 51% 129/252 [09:07<08:54,  4.35s/it]\u001b[A\n",
            " 52% 130/252 [09:11<08:37,  4.24s/it]\u001b[A\n",
            " 52% 131/252 [09:16<08:28,  4.20s/it]\u001b[A\n",
            " 52% 132/252 [09:20<08:34,  4.29s/it]\u001b[A\n",
            " 53% 133/252 [09:24<08:19,  4.20s/it]\u001b[A\n",
            " 53% 134/252 [09:29<08:24,  4.27s/it]\u001b[A\n",
            " 54% 135/252 [09:33<08:28,  4.34s/it]\u001b[A\n",
            " 54% 136/252 [09:37<08:11,  4.23s/it]\u001b[A\n",
            " 54% 137/252 [09:41<08:00,  4.18s/it]\u001b[A\n",
            " 55% 138/252 [09:46<08:12,  4.32s/it]\u001b[A\n",
            " 55% 139/252 [09:50<08:07,  4.31s/it]\u001b[A\n",
            " 56% 140/252 [09:54<07:54,  4.23s/it]\u001b[A\n",
            " 56% 141/252 [09:59<08:03,  4.36s/it]\u001b[A\n",
            " 56% 142/252 [10:03<07:48,  4.25s/it]\u001b[A\n",
            " 57% 143/252 [10:07<07:36,  4.18s/it]\u001b[A\n",
            " 57% 144/252 [10:12<08:00,  4.45s/it]\u001b[A\n",
            " 58% 145/252 [10:16<07:43,  4.33s/it]\u001b[A\n",
            " 58% 146/252 [10:20<07:29,  4.24s/it]\u001b[A\n",
            " 58% 147/252 [10:25<07:38,  4.37s/it]\u001b[A\n",
            " 59% 148/252 [10:29<07:24,  4.27s/it]\u001b[A\n",
            " 59% 149/252 [10:33<07:22,  4.30s/it]\u001b[A\n",
            " 60% 150/252 [10:38<07:28,  4.40s/it]\u001b[A\n",
            " 60% 151/252 [10:42<07:12,  4.28s/it]\u001b[A\n",
            " 60% 152/252 [10:46<07:00,  4.20s/it]\u001b[A\n",
            " 61% 153/252 [10:50<07:11,  4.36s/it]\u001b[A\n",
            " 61% 154/252 [10:54<06:56,  4.25s/it]\u001b[A\n",
            " 62% 155/252 [10:59<06:55,  4.28s/it]\u001b[A\n",
            " 62% 156/252 [11:03<07:05,  4.43s/it]\u001b[A\n",
            " 62% 157/252 [11:07<06:49,  4.31s/it]\u001b[A\n",
            " 63% 158/252 [11:11<06:36,  4.21s/it]\u001b[A\n",
            " 63% 159/252 [11:16<06:45,  4.36s/it]\u001b[A\n",
            " 63% 160/252 [11:20<06:31,  4.25s/it]\u001b[A\n",
            " 64% 161/252 [11:24<06:27,  4.26s/it]\u001b[A\n",
            " 64% 162/252 [11:29<06:36,  4.40s/it]\u001b[A\n",
            " 65% 163/252 [11:33<06:20,  4.27s/it]\u001b[A\n",
            " 65% 164/252 [11:37<06:08,  4.19s/it]\u001b[A\n",
            " 65% 165/252 [11:42<06:16,  4.33s/it]\u001b[A\n",
            " 66% 166/252 [11:46<06:10,  4.31s/it]\u001b[A\n",
            " 66% 167/252 [11:50<05:57,  4.20s/it]\u001b[A\n",
            " 67% 168/252 [11:55<06:05,  4.35s/it]\u001b[A\n",
            " 67% 169/252 [11:59<05:53,  4.25s/it]\u001b[A\n",
            " 67% 170/252 [12:03<05:41,  4.17s/it]\u001b[A\n",
            " 68% 171/252 [12:08<05:56,  4.40s/it]\u001b[A\n",
            " 68% 172/252 [12:12<05:41,  4.27s/it]\u001b[A\n",
            " 69% 173/252 [12:16<05:31,  4.19s/it]\u001b[A\n",
            " 69% 174/252 [12:20<05:36,  4.32s/it]\u001b[A\n",
            " 69% 175/252 [12:24<05:24,  4.22s/it]\u001b[A\n",
            " 70% 176/252 [12:29<05:22,  4.25s/it]\u001b[A\n",
            " 70% 177/252 [12:33<05:27,  4.37s/it]\u001b[A\n",
            " 71% 178/252 [12:37<05:14,  4.25s/it]\u001b[A\n",
            " 71% 179/252 [12:41<05:03,  4.16s/it]\u001b[A\n",
            " 71% 180/252 [12:46<05:07,  4.27s/it]\u001b[A\n",
            " 72% 181/252 [12:50<05:04,  4.28s/it]\u001b[A\n",
            " 72% 182/252 [12:54<04:52,  4.19s/it]\u001b[A\n",
            " 73% 183/252 [12:58<04:54,  4.26s/it]\u001b[A\n",
            " 73% 184/252 [13:03<04:48,  4.24s/it]\u001b[A\n",
            " 73% 185/252 [13:07<04:39,  4.18s/it]\u001b[A\n",
            " 74% 186/252 [13:11<04:45,  4.33s/it]\u001b[A\n",
            " 74% 187/252 [13:15<04:38,  4.28s/it]\u001b[A\n",
            " 75% 188/252 [13:19<04:28,  4.20s/it]\u001b[A\n",
            " 75% 189/252 [13:24<04:24,  4.21s/it]\u001b[A\n",
            " 75% 190/252 [13:28<04:23,  4.25s/it]\u001b[A\n",
            " 76% 191/252 [13:32<04:19,  4.26s/it]\u001b[A\n",
            " 76% 192/252 [13:37<04:16,  4.27s/it]\u001b[A\n",
            " 77% 193/252 [13:41<04:13,  4.29s/it]\u001b[A\n",
            " 77% 194/252 [13:45<04:03,  4.20s/it]\u001b[A\n",
            " 77% 195/252 [13:49<03:58,  4.18s/it]\u001b[A\n",
            " 78% 196/252 [13:54<04:05,  4.38s/it]\u001b[A\n",
            " 78% 197/252 [13:58<03:54,  4.27s/it]\u001b[A\n",
            " 79% 198/252 [14:02<03:48,  4.23s/it]\u001b[A\n",
            " 79% 199/252 [14:07<03:48,  4.31s/it]\u001b[A\n",
            " 79% 200/252 [14:11<03:39,  4.22s/it]\u001b[A\n",
            " 80% 201/252 [14:15<03:38,  4.29s/it]\u001b[A\n",
            " 80% 202/252 [14:20<03:38,  4.37s/it]\u001b[A\n",
            " 81% 203/252 [14:24<03:28,  4.26s/it]\u001b[A\n",
            " 81% 204/252 [14:28<03:22,  4.21s/it]\u001b[A\n",
            " 81% 205/252 [14:32<03:23,  4.32s/it]\u001b[A\n",
            " 82% 206/252 [14:37<03:18,  4.31s/it]\u001b[A\n",
            " 82% 207/252 [14:41<03:10,  4.24s/it]\u001b[A\n",
            " 83% 208/252 [14:45<03:11,  4.35s/it]\u001b[A\n",
            " 83% 209/252 [14:49<03:02,  4.24s/it]\u001b[A\n",
            " 83% 210/252 [14:53<02:55,  4.17s/it]\u001b[A\n",
            " 84% 211/252 [14:58<03:02,  4.44s/it]\u001b[A\n",
            " 84% 212/252 [15:02<02:52,  4.32s/it]\u001b[A\n",
            " 85% 213/252 [15:06<02:44,  4.22s/it]\u001b[A\n",
            " 85% 214/252 [15:11<02:45,  4.36s/it]\u001b[A\n",
            " 85% 215/252 [15:15<02:37,  4.25s/it]\u001b[A\n",
            " 86% 216/252 [15:19<02:33,  4.27s/it]\u001b[A\n",
            " 86% 217/252 [15:24<02:34,  4.40s/it]\u001b[A\n",
            " 87% 218/252 [15:28<02:25,  4.29s/it]\u001b[A\n",
            " 87% 219/252 [15:32<02:18,  4.21s/it]\u001b[A\n",
            " 87% 220/252 [15:37<02:20,  4.38s/it]\u001b[A\n",
            " 88% 221/252 [15:41<02:15,  4.36s/it]\u001b[A\n",
            " 88% 222/252 [15:45<02:07,  4.25s/it]\u001b[A\n",
            " 88% 223/252 [15:50<02:07,  4.39s/it]\u001b[A\n",
            " 89% 224/252 [15:54<01:59,  4.28s/it]\u001b[A\n",
            " 89% 225/252 [15:58<01:53,  4.21s/it]\u001b[A\n",
            " 90% 226/252 [16:03<01:55,  4.46s/it]\u001b[A\n",
            " 90% 227/252 [16:07<01:47,  4.31s/it]\u001b[A\n",
            " 90% 228/252 [16:11<01:41,  4.22s/it]\u001b[A\n",
            " 91% 229/252 [16:16<01:40,  4.39s/it]\u001b[A\n",
            " 91% 230/252 [16:20<01:34,  4.27s/it]\u001b[A\n",
            " 92% 231/252 [16:24<01:29,  4.28s/it]\u001b[A\n",
            " 92% 232/252 [16:29<01:28,  4.43s/it]\u001b[A\n",
            " 92% 233/252 [16:33<01:21,  4.30s/it]\u001b[A\n",
            " 93% 234/252 [16:37<01:16,  4.23s/it]\u001b[A\n",
            " 93% 235/252 [16:42<01:14,  4.37s/it]\u001b[A\n",
            " 94% 236/252 [16:46<01:08,  4.26s/it]\u001b[A\n",
            " 94% 237/252 [16:50<01:04,  4.28s/it]\u001b[A\n",
            " 94% 238/252 [16:55<01:01,  4.42s/it]\u001b[A\n",
            " 95% 239/252 [16:59<00:55,  4.30s/it]\u001b[A\n",
            " 95% 240/252 [17:03<00:50,  4.22s/it]\u001b[A\n",
            " 96% 241/252 [17:08<00:48,  4.45s/it]\u001b[A\n",
            " 96% 242/252 [17:12<00:43,  4.31s/it]\u001b[A\n",
            " 96% 243/252 [17:16<00:37,  4.21s/it]\u001b[A\n",
            " 97% 244/252 [17:20<00:34,  4.35s/it]\u001b[A\n",
            " 97% 245/252 [17:24<00:29,  4.25s/it]\u001b[A\n",
            " 98% 246/252 [17:29<00:25,  4.28s/it]\u001b[A\n",
            " 98% 247/252 [17:33<00:21,  4.40s/it]\u001b[A\n",
            " 98% 248/252 [17:37<00:17,  4.28s/it]\u001b[A\n",
            " 99% 249/252 [17:41<00:12,  4.21s/it]\u001b[A\n",
            " 99% 250/252 [17:46<00:08,  4.32s/it]\u001b[A\n",
            "100% 251/252 [17:50<00:04,  4.33s/it]\u001b[A\n",
            "100% 252/252 [17:54<00:00,  4.12s/it]\u001b[A\n",
            "\n",
            "Downloading builder script: 100% 6.27k/6.27k [00:00<00:00, 23.3MB/s]\n",
            "INFO:absl:Using default tokenizer.\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.04758242517709732, 'eval_rouge1': 27.527, 'eval_rouge2': 22.2935, 'eval_rougeL': 27.4935, 'eval_rougeLsum': 27.5265, 'eval_runtime': 1082.7086, 'eval_samples_per_second': 1.86, 'eval_steps_per_second': 0.233, 'epoch': 0.04}\n",
            "  1% 100/6798 [18:39<40:44,  2.74it/s]\n",
            "100% 252/252 [17:57<00:00,  4.12s/it]\u001b[A\n",
            "                                     \u001b[A/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n",
            "{'loss': 0.0111, 'grad_norm': 1.1707924604415894, 'learning_rate': 1.942630185348632e-05, 'epoch': 0.09}\n",
            "  3% 200/6798 [20:12<44:53,  2.45it/s]\n",
            "  0% 0/252 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/252 [00:05<10:39,  2.56s/it]\u001b[A\n",
            "  1% 3/252 [00:10<14:59,  3.61s/it]\u001b[A\n",
            "  2% 4/252 [00:14<15:39,  3.79s/it]\u001b[A\n",
            "  2% 5/252 [00:18<16:01,  3.89s/it]\u001b[A\n",
            "  2% 6/252 [00:23<17:14,  4.20s/it]\u001b[A\n",
            "  3% 7/252 [00:27<17:01,  4.17s/it]\u001b[A\n",
            "  3% 8/252 [00:31<17:14,  4.24s/it]\u001b[A\n",
            "  4% 9/252 [00:36<17:49,  4.40s/it]\u001b[A\n",
            "  4% 10/252 [00:40<17:19,  4.29s/it]\u001b[A\n",
            "  4% 11/252 [00:44<17:00,  4.23s/it]\u001b[A\n",
            "  5% 12/252 [00:49<17:39,  4.41s/it]\u001b[A\n",
            "  5% 13/252 [00:53<17:16,  4.34s/it]\u001b[A\n",
            "  6% 14/252 [00:58<17:18,  4.36s/it]\u001b[A\n",
            "  6% 15/252 [01:02<17:50,  4.51s/it]\u001b[A\n",
            "  6% 16/252 [01:06<17:13,  4.38s/it]\u001b[A\n",
            "  7% 17/252 [01:11<16:46,  4.28s/it]\u001b[A\n",
            "  7% 18/252 [01:15<17:14,  4.42s/it]\u001b[A\n",
            "  8% 19/252 [01:20<17:07,  4.41s/it]\u001b[A\n",
            "  8% 20/252 [01:24<16:42,  4.32s/it]\u001b[A\n",
            "  8% 21/252 [01:29<17:11,  4.47s/it]\u001b[A\n",
            "  9% 22/252 [01:33<16:37,  4.34s/it]\u001b[A\n",
            "  9% 23/252 [01:37<16:35,  4.35s/it]\u001b[A\n",
            " 10% 24/252 [01:42<16:58,  4.47s/it]\u001b[A\n",
            " 10% 25/252 [01:46<16:25,  4.34s/it]\u001b[A\n",
            " 10% 26/252 [01:50<16:02,  4.26s/it]\u001b[A\n",
            " 11% 27/252 [01:55<16:37,  4.44s/it]\u001b[A\n",
            " 11% 28/252 [01:59<16:27,  4.41s/it]\u001b[A\n",
            " 12% 29/252 [02:03<15:59,  4.30s/it]\u001b[A\n",
            " 12% 30/252 [02:08<16:27,  4.45s/it]\u001b[A\n",
            " 12% 31/252 [02:12<15:54,  4.32s/it]\u001b[A\n",
            " 13% 32/252 [02:16<15:32,  4.24s/it]\u001b[A\n",
            " 13% 33/252 [02:21<16:25,  4.50s/it]\u001b[A\n",
            " 13% 34/252 [02:25<15:51,  4.37s/it]\u001b[A\n",
            " 14% 35/252 [02:29<15:26,  4.27s/it]\u001b[A\n",
            " 14% 36/252 [02:34<15:53,  4.42s/it]\u001b[A\n",
            " 15% 37/252 [02:38<15:24,  4.30s/it]\u001b[A\n",
            " 15% 38/252 [02:42<15:24,  4.32s/it]\u001b[A\n",
            " 15% 39/252 [02:47<15:48,  4.45s/it]\u001b[A\n",
            " 16% 40/252 [02:51<15:17,  4.33s/it]\u001b[A\n",
            " 16% 41/252 [02:55<14:54,  4.24s/it]\u001b[A\n",
            " 17% 42/252 [03:00<15:45,  4.50s/it]\u001b[A\n",
            " 17% 43/252 [03:04<15:11,  4.36s/it]\u001b[A\n",
            " 17% 44/252 [03:08<14:46,  4.26s/it]\u001b[A\n",
            " 18% 45/252 [03:13<15:15,  4.42s/it]\u001b[A\n",
            " 18% 46/252 [03:17<14:47,  4.31s/it]\u001b[A\n",
            " 19% 47/252 [03:22<14:50,  4.35s/it]\u001b[A\n",
            " 19% 48/252 [03:26<15:16,  4.49s/it]\u001b[A\n",
            " 19% 49/252 [03:30<14:44,  4.36s/it]\u001b[A\n",
            " 20% 50/252 [03:34<14:20,  4.26s/it]\u001b[A\n",
            " 20% 51/252 [03:39<14:46,  4.41s/it]\u001b[A\n",
            " 21% 52/252 [03:44<14:38,  4.39s/it]\u001b[A\n",
            " 21% 53/252 [03:48<14:14,  4.29s/it]\u001b[A\n",
            " 21% 54/252 [03:53<14:45,  4.47s/it]\u001b[A\n",
            " 22% 55/252 [03:57<14:20,  4.37s/it]\u001b[A\n",
            " 22% 56/252 [04:01<13:58,  4.28s/it]\u001b[A\n",
            " 23% 57/252 [04:06<14:44,  4.53s/it]\u001b[A\n",
            " 23% 58/252 [04:10<14:11,  4.39s/it]\u001b[A\n",
            " 23% 59/252 [04:14<13:49,  4.30s/it]\u001b[A\n",
            " 24% 60/252 [04:19<14:10,  4.43s/it]\u001b[A\n",
            " 24% 61/252 [04:23<14:03,  4.42s/it]\u001b[A\n",
            " 25% 62/252 [04:27<13:40,  4.32s/it]\u001b[A\n",
            " 25% 63/252 [04:32<13:57,  4.43s/it]\u001b[A\n",
            " 25% 64/252 [04:36<13:30,  4.31s/it]\u001b[A\n",
            " 26% 65/252 [04:40<13:11,  4.23s/it]\u001b[A\n",
            " 26% 66/252 [04:45<13:55,  4.49s/it]\u001b[A\n",
            " 27% 67/252 [04:49<13:24,  4.35s/it]\u001b[A\n",
            " 27% 68/252 [04:53<13:04,  4.26s/it]\u001b[A\n",
            " 27% 69/252 [04:58<13:28,  4.42s/it]\u001b[A\n",
            " 28% 70/252 [05:02<13:04,  4.31s/it]\u001b[A\n",
            " 28% 71/252 [05:06<13:03,  4.33s/it]\u001b[A\n",
            " 29% 72/252 [05:11<13:17,  4.43s/it]\u001b[A\n",
            " 29% 73/252 [05:15<12:52,  4.31s/it]\u001b[A\n",
            " 29% 74/252 [05:19<12:32,  4.23s/it]\u001b[A\n",
            " 30% 75/252 [05:24<12:57,  4.39s/it]\u001b[A\n",
            " 30% 76/252 [05:28<12:52,  4.39s/it]\u001b[A\n",
            " 31% 77/252 [05:32<12:30,  4.29s/it]\u001b[A\n",
            " 31% 78/252 [05:37<12:51,  4.43s/it]\u001b[A\n",
            " 31% 79/252 [05:41<12:26,  4.32s/it]\u001b[A\n",
            " 32% 80/252 [05:46<12:26,  4.34s/it]\u001b[A\n",
            " 32% 81/252 [05:50<12:39,  4.44s/it]\u001b[A\n",
            " 33% 82/252 [05:54<12:16,  4.33s/it]\u001b[A\n",
            " 33% 83/252 [05:58<11:58,  4.25s/it]\u001b[A\n",
            " 33% 84/252 [06:03<12:17,  4.39s/it]\u001b[A\n",
            " 34% 85/252 [06:07<12:09,  4.37s/it]\u001b[A\n",
            " 34% 86/252 [06:11<11:48,  4.27s/it]\u001b[A\n",
            " 35% 87/252 [06:16<12:06,  4.40s/it]\u001b[A\n",
            " 35% 88/252 [06:20<11:47,  4.31s/it]\u001b[A\n",
            " 35% 89/252 [06:24<11:30,  4.24s/it]\u001b[A\n",
            " 36% 90/252 [06:29<12:07,  4.49s/it]\u001b[A\n",
            " 36% 91/252 [06:33<11:40,  4.35s/it]\u001b[A\n",
            " 37% 92/252 [06:37<11:21,  4.26s/it]\u001b[A\n",
            " 37% 93/252 [06:42<11:38,  4.39s/it]\u001b[A\n",
            " 37% 94/252 [06:46<11:17,  4.29s/it]\u001b[A\n",
            " 38% 95/252 [06:51<11:16,  4.31s/it]\u001b[A\n",
            " 38% 96/252 [06:55<11:29,  4.42s/it]\u001b[A\n",
            " 38% 97/252 [06:59<11:05,  4.29s/it]\u001b[A\n",
            " 39% 98/252 [07:03<10:47,  4.21s/it]\u001b[A\n",
            " 39% 99/252 [07:08<11:08,  4.37s/it]\u001b[A\n",
            " 40% 100/252 [07:12<11:01,  4.35s/it]\u001b[A\n",
            " 40% 101/252 [07:16<10:42,  4.26s/it]\u001b[A\n",
            " 40% 102/252 [07:21<10:56,  4.37s/it]\u001b[A\n",
            " 41% 103/252 [07:25<10:37,  4.28s/it]\u001b[A\n",
            " 41% 104/252 [07:29<10:20,  4.19s/it]\u001b[A\n",
            " 42% 105/252 [07:34<10:53,  4.44s/it]\u001b[A\n",
            " 42% 106/252 [07:38<10:29,  4.31s/it]\u001b[A\n",
            " 42% 107/252 [07:42<10:10,  4.21s/it]\u001b[A\n",
            " 43% 108/252 [07:47<10:28,  4.36s/it]\u001b[A\n",
            " 43% 109/252 [07:51<10:21,  4.34s/it]\u001b[A\n",
            " 44% 110/252 [07:55<10:03,  4.25s/it]\u001b[A\n",
            " 44% 111/252 [08:00<10:21,  4.40s/it]\u001b[A\n",
            " 44% 112/252 [08:04<09:59,  4.28s/it]\u001b[A\n",
            " 45% 113/252 [08:08<09:45,  4.21s/it]\u001b[A\n",
            " 45% 114/252 [08:13<10:17,  4.47s/it]\u001b[A\n",
            " 46% 115/252 [08:17<09:54,  4.34s/it]\u001b[A\n",
            " 46% 116/252 [08:21<09:36,  4.24s/it]\u001b[A\n",
            " 46% 117/252 [08:26<09:53,  4.40s/it]\u001b[A\n",
            " 47% 118/252 [08:30<09:47,  4.39s/it]\u001b[A\n",
            " 47% 119/252 [08:34<09:27,  4.27s/it]\u001b[A\n",
            " 48% 120/252 [08:39<09:40,  4.40s/it]\u001b[A\n",
            " 48% 121/252 [08:43<09:20,  4.28s/it]\u001b[A\n",
            " 48% 122/252 [08:47<09:17,  4.28s/it]\u001b[A\n",
            " 49% 123/252 [08:52<09:28,  4.41s/it]\u001b[A\n",
            " 49% 124/252 [08:56<09:11,  4.30s/it]\u001b[A\n",
            " 50% 125/252 [09:00<08:56,  4.22s/it]\u001b[A\n",
            " 50% 126/252 [09:05<09:13,  4.39s/it]\u001b[A\n",
            " 50% 127/252 [09:09<09:05,  4.36s/it]\u001b[A\n",
            " 51% 128/252 [09:13<08:47,  4.25s/it]\u001b[A\n",
            " 51% 129/252 [09:18<09:02,  4.41s/it]\u001b[A\n",
            " 52% 130/252 [09:22<08:42,  4.29s/it]\u001b[A\n",
            " 52% 131/252 [09:26<08:29,  4.21s/it]\u001b[A\n",
            " 52% 132/252 [09:31<08:56,  4.47s/it]\u001b[A\n",
            " 53% 133/252 [09:35<08:35,  4.33s/it]\u001b[A\n",
            " 53% 134/252 [09:39<08:19,  4.23s/it]\u001b[A\n",
            " 54% 135/252 [09:44<08:32,  4.38s/it]\u001b[A\n",
            " 54% 136/252 [09:48<08:15,  4.27s/it]\u001b[A\n",
            " 54% 137/252 [09:52<08:12,  4.28s/it]\u001b[A\n",
            " 55% 138/252 [09:57<08:26,  4.45s/it]\u001b[A\n",
            " 55% 139/252 [10:01<08:08,  4.32s/it]\u001b[A\n",
            " 56% 140/252 [10:05<07:53,  4.22s/it]\u001b[A\n",
            " 56% 141/252 [10:10<08:17,  4.48s/it]\u001b[A\n",
            " 56% 142/252 [10:14<07:57,  4.34s/it]\u001b[A\n",
            " 57% 143/252 [10:18<07:42,  4.24s/it]\u001b[A\n",
            " 57% 144/252 [10:23<07:55,  4.40s/it]\u001b[A\n",
            " 58% 145/252 [10:27<07:39,  4.30s/it]\u001b[A\n",
            " 58% 146/252 [10:31<07:36,  4.31s/it]\u001b[A\n",
            " 58% 147/252 [10:36<07:46,  4.45s/it]\u001b[A\n",
            " 59% 148/252 [10:40<07:29,  4.32s/it]\u001b[A\n",
            " 59% 149/252 [10:44<07:16,  4.23s/it]\u001b[A\n",
            " 60% 150/252 [10:49<07:38,  4.49s/it]\u001b[A\n",
            " 60% 151/252 [10:53<07:18,  4.35s/it]\u001b[A\n",
            " 60% 152/252 [10:57<07:06,  4.26s/it]\u001b[A\n",
            " 61% 153/252 [11:02<07:16,  4.41s/it]\u001b[A\n",
            " 61% 154/252 [11:06<06:59,  4.28s/it]\u001b[A\n",
            " 62% 155/252 [11:10<06:56,  4.30s/it]\u001b[A\n",
            " 62% 156/252 [11:15<07:06,  4.44s/it]\u001b[A\n",
            " 62% 157/252 [11:19<06:51,  4.33s/it]\u001b[A\n",
            " 63% 158/252 [11:23<06:38,  4.24s/it]\u001b[A\n",
            " 63% 159/252 [11:28<06:50,  4.42s/it]\u001b[A\n",
            " 63% 160/252 [11:32<06:43,  4.39s/it]\u001b[A\n",
            " 64% 161/252 [11:36<06:28,  4.27s/it]\u001b[A\n",
            " 64% 162/252 [11:41<06:36,  4.40s/it]\u001b[A\n",
            " 65% 163/252 [11:45<06:20,  4.28s/it]\u001b[A\n",
            " 65% 164/252 [11:49<06:16,  4.28s/it]\u001b[A\n",
            " 65% 165/252 [11:54<06:24,  4.42s/it]\u001b[A\n",
            " 66% 166/252 [11:58<06:09,  4.30s/it]\u001b[A\n",
            " 66% 167/252 [12:02<05:57,  4.21s/it]\u001b[A\n",
            " 67% 168/252 [12:07<06:06,  4.36s/it]\u001b[A\n",
            " 67% 169/252 [12:11<06:01,  4.35s/it]\u001b[A\n",
            " 67% 170/252 [12:15<05:48,  4.25s/it]\u001b[A\n",
            " 68% 171/252 [12:20<05:56,  4.40s/it]\u001b[A\n",
            " 68% 172/252 [12:24<05:42,  4.28s/it]\u001b[A\n",
            " 69% 173/252 [12:28<05:32,  4.20s/it]\u001b[A\n",
            " 69% 174/252 [12:33<05:47,  4.45s/it]\u001b[A\n",
            " 69% 175/252 [12:37<05:32,  4.32s/it]\u001b[A\n",
            " 70% 176/252 [12:41<05:21,  4.22s/it]\u001b[A\n",
            " 70% 177/252 [12:45<05:27,  4.37s/it]\u001b[A\n",
            " 71% 178/252 [12:49<05:14,  4.25s/it]\u001b[A\n",
            " 71% 179/252 [12:54<05:11,  4.27s/it]\u001b[A\n",
            " 71% 180/252 [12:59<05:18,  4.42s/it]\u001b[A\n",
            " 72% 181/252 [13:02<05:04,  4.29s/it]\u001b[A\n",
            " 72% 182/252 [13:06<04:53,  4.20s/it]\u001b[A\n",
            " 73% 183/252 [13:11<05:06,  4.44s/it]\u001b[A\n",
            " 73% 184/252 [13:15<04:52,  4.31s/it]\u001b[A\n",
            " 73% 185/252 [13:19<04:42,  4.21s/it]\u001b[A\n",
            " 74% 186/252 [13:24<04:49,  4.39s/it]\u001b[A\n",
            " 74% 187/252 [13:28<04:38,  4.28s/it]\u001b[A\n",
            " 75% 188/252 [13:33<04:35,  4.30s/it]\u001b[A\n",
            " 75% 189/252 [13:37<04:38,  4.41s/it]\u001b[A\n",
            " 75% 190/252 [13:41<04:26,  4.30s/it]\u001b[A\n",
            " 76% 191/252 [13:45<04:17,  4.22s/it]\u001b[A\n",
            " 76% 192/252 [13:50<04:23,  4.39s/it]\u001b[A\n",
            " 77% 193/252 [13:55<04:18,  4.38s/it]\u001b[A\n",
            " 77% 194/252 [13:59<04:07,  4.27s/it]\u001b[A\n",
            " 77% 195/252 [14:03<04:12,  4.42s/it]\u001b[A\n",
            " 78% 196/252 [14:07<04:01,  4.31s/it]\u001b[A\n",
            " 78% 197/252 [14:11<03:53,  4.24s/it]\u001b[A\n",
            " 79% 198/252 [14:17<04:03,  4.50s/it]\u001b[A\n",
            " 79% 199/252 [14:21<03:51,  4.37s/it]\u001b[A\n",
            " 79% 200/252 [14:25<03:41,  4.27s/it]\u001b[A\n",
            " 80% 201/252 [14:29<03:45,  4.42s/it]\u001b[A\n",
            " 80% 202/252 [14:33<03:34,  4.29s/it]\u001b[A\n",
            " 81% 203/252 [14:38<03:30,  4.30s/it]\u001b[A\n",
            " 81% 204/252 [14:42<03:32,  4.43s/it]\u001b[A\n",
            " 81% 205/252 [14:46<03:22,  4.30s/it]\u001b[A\n",
            " 82% 206/252 [14:50<03:13,  4.21s/it]\u001b[A\n",
            " 82% 207/252 [14:55<03:16,  4.37s/it]\u001b[A\n",
            " 83% 208/252 [14:59<03:07,  4.26s/it]\u001b[A\n",
            " 83% 209/252 [15:04<03:03,  4.27s/it]\u001b[A\n",
            " 83% 210/252 [15:08<03:05,  4.41s/it]\u001b[A\n",
            " 84% 211/252 [15:12<02:55,  4.28s/it]\u001b[A\n",
            " 84% 212/252 [15:16<02:48,  4.20s/it]\u001b[A\n",
            " 85% 213/252 [15:21<02:49,  4.35s/it]\u001b[A\n",
            " 85% 214/252 [15:25<02:44,  4.34s/it]\u001b[A\n",
            " 85% 215/252 [15:29<02:36,  4.23s/it]\u001b[A\n",
            " 86% 216/252 [15:34<02:37,  4.37s/it]\u001b[A\n",
            " 86% 217/252 [15:38<02:29,  4.27s/it]\u001b[A\n",
            " 87% 218/252 [15:42<02:26,  4.29s/it]\u001b[A\n",
            " 87% 219/252 [15:47<02:26,  4.43s/it]\u001b[A\n",
            " 87% 220/252 [15:51<02:17,  4.31s/it]\u001b[A\n",
            " 88% 221/252 [15:55<02:11,  4.23s/it]\u001b[A\n",
            " 88% 222/252 [16:00<02:11,  4.38s/it]\u001b[A\n",
            " 88% 223/252 [16:04<02:06,  4.35s/it]\u001b[A\n",
            " 89% 224/252 [16:08<01:58,  4.24s/it]\u001b[A\n",
            " 89% 225/252 [16:13<01:58,  4.39s/it]\u001b[A\n",
            " 90% 226/252 [16:17<01:51,  4.28s/it]\u001b[A\n",
            " 90% 227/252 [16:21<01:47,  4.29s/it]\u001b[A\n",
            " 90% 228/252 [16:26<01:46,  4.42s/it]\u001b[A\n",
            " 91% 229/252 [16:30<01:38,  4.29s/it]\u001b[A\n",
            " 91% 230/252 [16:34<01:32,  4.20s/it]\u001b[A\n",
            " 92% 231/252 [16:39<01:31,  4.34s/it]\u001b[A\n",
            " 92% 232/252 [16:43<01:26,  4.33s/it]\u001b[A\n",
            " 92% 233/252 [16:47<01:20,  4.23s/it]\u001b[A\n",
            " 93% 234/252 [16:52<01:18,  4.39s/it]\u001b[A\n",
            " 93% 235/252 [16:56<01:12,  4.28s/it]\u001b[A\n",
            " 94% 236/252 [17:00<01:07,  4.20s/it]\u001b[A\n",
            " 94% 237/252 [17:05<01:06,  4.45s/it]\u001b[A\n",
            " 94% 238/252 [17:09<01:00,  4.30s/it]\u001b[A\n",
            " 95% 239/252 [17:13<00:54,  4.20s/it]\u001b[A\n",
            " 95% 240/252 [17:17<00:52,  4.36s/it]\u001b[A\n",
            " 96% 241/252 [17:22<00:47,  4.35s/it]\u001b[A\n",
            " 96% 242/252 [17:26<00:42,  4.24s/it]\u001b[A\n",
            " 96% 243/252 [17:30<00:39,  4.37s/it]\u001b[A\n",
            " 97% 244/252 [17:34<00:34,  4.26s/it]\u001b[A\n",
            " 97% 245/252 [17:38<00:29,  4.19s/it]\u001b[A\n",
            " 98% 246/252 [17:43<00:25,  4.32s/it]\u001b[A\n",
            " 98% 247/252 [17:47<00:21,  4.32s/it]\u001b[A\n",
            " 98% 248/252 [17:51<00:16,  4.22s/it]\u001b[A\n",
            " 99% 249/252 [17:56<00:13,  4.37s/it]\u001b[A\n",
            " 99% 250/252 [18:00<00:08,  4.26s/it]\u001b[A\n",
            "100% 251/252 [18:04<00:04,  4.28s/it]\u001b[A\n",
            "100% 252/252 [18:09<00:00,  4.30s/it]\u001b[AINFO:absl:Using default tokenizer.\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.024521509185433388, 'eval_rouge1': 25.1712, 'eval_rouge2': 20.3404, 'eval_rougeL': 25.1425, 'eval_rougeLsum': 25.156, 'eval_runtime': 1097.5254, 'eval_samples_per_second': 1.835, 'eval_steps_per_second': 0.23, 'epoch': 0.09}\n",
            "  3% 200/6798 [38:29<44:53,  2.45it/s]\n",
            "100% 252/252 [18:10<00:00,  4.30s/it]\u001b[A\n",
            "{'loss': 0.005, 'grad_norm': 1.025448203086853, 'learning_rate': 1.9132097675786996e-05, 'epoch': 0.13}\n",
            "  4% 300/6798 [40:15<43:11,  2.51it/s]\n",
            "  0% 0/252 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/252 [00:04<09:13,  2.21s/it]\u001b[A\n",
            "  1% 3/252 [00:08<12:35,  3.03s/it]\u001b[A\n",
            "  2% 4/252 [00:12<14:32,  3.52s/it]\u001b[A\n",
            "  2% 5/252 [00:17<15:41,  3.81s/it]\u001b[A\n",
            "  2% 6/252 [00:21<16:09,  3.94s/it]\u001b[A\n",
            "  3% 7/252 [00:25<16:16,  3.98s/it]\u001b[A\n",
            "  3% 8/252 [00:29<16:35,  4.08s/it]\u001b[A\n",
            "  4% 9/252 [00:34<16:49,  4.15s/it]\u001b[A\n",
            "  4% 10/252 [00:38<16:56,  4.20s/it]\u001b[A\n",
            "  4% 11/252 [00:42<16:58,  4.22s/it]\u001b[A\n",
            "  5% 12/252 [00:47<16:59,  4.25s/it]\u001b[A\n",
            "  5% 13/252 [00:51<16:36,  4.17s/it]\u001b[A\n",
            "  6% 14/252 [00:55<16:32,  4.17s/it]\u001b[A\n",
            "  6% 15/252 [01:00<17:14,  4.36s/it]\u001b[A\n",
            "  6% 16/252 [01:04<16:45,  4.26s/it]\u001b[A\n",
            "  7% 17/252 [01:08<16:38,  4.25s/it]\u001b[A\n",
            "  7% 18/252 [01:12<16:49,  4.32s/it]\u001b[A\n",
            "  8% 19/252 [01:16<16:27,  4.24s/it]\u001b[A\n",
            "  8% 20/252 [01:21<16:45,  4.33s/it]\u001b[A\n",
            "  8% 21/252 [01:25<16:54,  4.39s/it]\u001b[A\n",
            "  9% 22/252 [01:29<16:24,  4.28s/it]\u001b[A\n",
            "  9% 23/252 [01:34<16:14,  4.26s/it]\u001b[A\n",
            " 10% 24/252 [01:38<16:35,  4.36s/it]\u001b[A\n",
            " 10% 25/252 [01:43<16:28,  4.35s/it]\u001b[A\n",
            " 10% 26/252 [01:47<16:12,  4.30s/it]\u001b[A\n",
            " 11% 27/252 [01:51<16:25,  4.38s/it]\u001b[A\n",
            " 11% 28/252 [01:55<16:00,  4.29s/it]\u001b[A\n",
            " 12% 29/252 [02:00<15:48,  4.26s/it]\u001b[A\n",
            " 12% 30/252 [02:04<16:27,  4.45s/it]\u001b[A\n",
            " 12% 31/252 [02:09<15:57,  4.33s/it]\u001b[A\n",
            " 13% 32/252 [02:13<15:43,  4.29s/it]\u001b[A\n",
            " 13% 33/252 [02:17<16:03,  4.40s/it]\u001b[A\n",
            " 13% 34/252 [02:21<15:34,  4.29s/it]\u001b[A\n",
            " 14% 35/252 [02:26<15:47,  4.37s/it]\u001b[A\n",
            " 14% 36/252 [02:31<15:57,  4.43s/it]\u001b[A\n",
            " 15% 37/252 [02:35<15:29,  4.33s/it]\u001b[A\n",
            " 15% 38/252 [02:39<15:17,  4.29s/it]\u001b[A\n",
            " 15% 39/252 [02:43<15:32,  4.38s/it]\u001b[A\n",
            " 16% 40/252 [02:48<15:28,  4.38s/it]\u001b[A\n",
            " 16% 41/252 [02:52<15:14,  4.33s/it]\u001b[A\n",
            " 17% 42/252 [02:57<15:29,  4.43s/it]\u001b[A\n",
            " 17% 43/252 [03:01<15:06,  4.34s/it]\u001b[A\n",
            " 17% 44/252 [03:05<14:55,  4.30s/it]\u001b[A\n",
            " 18% 45/252 [03:10<15:27,  4.48s/it]\u001b[A\n",
            " 18% 46/252 [03:14<14:55,  4.35s/it]\u001b[A\n",
            " 19% 47/252 [03:18<14:42,  4.31s/it]\u001b[A\n",
            " 19% 48/252 [03:23<14:51,  4.37s/it]\u001b[A\n",
            " 19% 49/252 [03:27<14:27,  4.28s/it]\u001b[A\n",
            " 20% 50/252 [03:31<14:36,  4.34s/it]\u001b[A\n",
            " 20% 51/252 [03:36<14:43,  4.40s/it]\u001b[A\n",
            " 21% 52/252 [03:40<14:16,  4.28s/it]\u001b[A\n",
            " 21% 53/252 [03:44<14:03,  4.24s/it]\u001b[A\n",
            " 21% 54/252 [03:48<14:20,  4.35s/it]\u001b[A\n",
            " 22% 55/252 [03:53<14:15,  4.34s/it]\u001b[A\n",
            " 22% 56/252 [03:57<14:02,  4.30s/it]\u001b[A\n",
            " 23% 57/252 [04:02<14:16,  4.39s/it]\u001b[A\n",
            " 23% 58/252 [04:06<13:51,  4.28s/it]\u001b[A\n",
            " 23% 59/252 [04:10<13:37,  4.24s/it]\u001b[A\n",
            " 24% 60/252 [04:15<14:18,  4.47s/it]\u001b[A\n",
            " 24% 61/252 [04:19<13:50,  4.35s/it]\u001b[A\n",
            " 25% 62/252 [04:23<13:35,  4.29s/it]\u001b[A\n",
            " 25% 63/252 [04:28<13:52,  4.40s/it]\u001b[A\n",
            " 25% 64/252 [04:32<13:28,  4.30s/it]\u001b[A\n",
            " 26% 65/252 [04:36<13:36,  4.36s/it]\u001b[A\n",
            " 26% 66/252 [04:41<13:42,  4.42s/it]\u001b[A\n",
            " 27% 67/252 [04:45<13:15,  4.30s/it]\u001b[A\n",
            " 27% 68/252 [04:49<12:59,  4.24s/it]\u001b[A\n",
            " 27% 69/252 [04:54<13:15,  4.35s/it]\u001b[A\n",
            " 28% 70/252 [04:58<13:10,  4.34s/it]\u001b[A\n",
            " 28% 71/252 [05:02<12:53,  4.28s/it]\u001b[A\n",
            " 29% 72/252 [05:07<13:06,  4.37s/it]\u001b[A\n",
            " 29% 73/252 [05:11<12:42,  4.26s/it]\u001b[A\n",
            " 29% 74/252 [05:15<12:28,  4.20s/it]\u001b[A\n",
            " 30% 75/252 [05:20<13:10,  4.47s/it]\u001b[A\n",
            " 30% 76/252 [05:24<12:43,  4.34s/it]\u001b[A\n",
            " 31% 77/252 [05:28<12:30,  4.29s/it]\u001b[A\n",
            " 31% 78/252 [05:33<12:45,  4.40s/it]\u001b[A\n",
            " 31% 79/252 [05:37<12:23,  4.30s/it]\u001b[A\n",
            " 32% 80/252 [05:41<12:28,  4.35s/it]\u001b[A\n",
            " 32% 81/252 [05:46<12:40,  4.44s/it]\u001b[A\n",
            " 33% 82/252 [05:50<12:15,  4.33s/it]\u001b[A\n",
            " 33% 83/252 [05:54<11:59,  4.25s/it]\u001b[A\n",
            " 33% 84/252 [05:59<12:18,  4.40s/it]\u001b[A\n",
            " 34% 85/252 [06:03<12:11,  4.38s/it]\u001b[A\n",
            " 34% 86/252 [06:07<11:54,  4.31s/it]\u001b[A\n",
            " 35% 87/252 [06:12<12:09,  4.42s/it]\u001b[A\n",
            " 35% 88/252 [06:16<11:47,  4.31s/it]\u001b[A\n",
            " 35% 89/252 [06:20<11:28,  4.22s/it]\u001b[A\n",
            " 36% 90/252 [06:25<12:06,  4.48s/it]\u001b[A\n",
            " 36% 91/252 [06:29<11:41,  4.36s/it]\u001b[A\n",
            " 37% 92/252 [06:33<11:24,  4.28s/it]\u001b[A\n",
            " 37% 93/252 [06:38<11:41,  4.41s/it]\u001b[A\n",
            " 37% 94/252 [06:42<11:20,  4.31s/it]\u001b[A\n",
            " 38% 95/252 [06:46<11:22,  4.34s/it]\u001b[A\n",
            " 38% 96/252 [06:51<11:32,  4.44s/it]\u001b[A\n",
            " 38% 97/252 [06:55<11:09,  4.32s/it]\u001b[A\n",
            " 39% 98/252 [06:59<10:55,  4.26s/it]\u001b[A\n",
            " 39% 99/252 [07:04<11:12,  4.40s/it]\u001b[A\n",
            " 40% 100/252 [07:08<10:51,  4.29s/it]\u001b[A\n",
            " 40% 101/252 [07:12<10:52,  4.32s/it]\u001b[A\n",
            " 40% 102/252 [07:17<11:06,  4.44s/it]\u001b[A\n",
            " 41% 103/252 [07:21<10:43,  4.32s/it]\u001b[A\n",
            " 41% 104/252 [07:25<10:27,  4.24s/it]\u001b[A\n",
            " 42% 105/252 [07:30<11:01,  4.50s/it]\u001b[A\n",
            " 42% 106/252 [07:34<10:34,  4.35s/it]\u001b[A\n",
            " 42% 107/252 [07:38<10:17,  4.26s/it]\u001b[A\n",
            " 43% 108/252 [07:43<10:33,  4.40s/it]\u001b[A\n",
            " 43% 109/252 [07:47<10:14,  4.29s/it]\u001b[A\n",
            " 44% 110/252 [07:51<09:57,  4.21s/it]\u001b[A\n",
            " 44% 111/252 [07:56<10:31,  4.48s/it]\u001b[A\n",
            " 44% 112/252 [08:00<10:10,  4.36s/it]\u001b[A\n",
            " 45% 113/252 [08:04<09:54,  4.27s/it]\u001b[A\n",
            " 45% 114/252 [08:09<10:07,  4.41s/it]\u001b[A\n",
            " 46% 115/252 [08:13<09:48,  4.30s/it]\u001b[A\n",
            " 46% 116/252 [08:18<09:49,  4.34s/it]\u001b[A\n",
            " 46% 117/252 [08:22<09:58,  4.43s/it]\u001b[A\n",
            " 47% 118/252 [08:26<09:38,  4.31s/it]\u001b[A\n",
            " 47% 119/252 [08:30<09:25,  4.25s/it]\u001b[A\n",
            " 48% 120/252 [08:35<09:39,  4.39s/it]\u001b[A\n",
            " 48% 121/252 [08:39<09:32,  4.37s/it]\u001b[A\n",
            " 48% 122/252 [08:43<09:13,  4.26s/it]\u001b[A\n",
            " 49% 123/252 [08:48<09:27,  4.40s/it]\u001b[A\n",
            " 49% 124/252 [08:52<09:07,  4.28s/it]\u001b[A\n",
            " 50% 125/252 [08:56<08:55,  4.21s/it]\u001b[A\n",
            " 50% 126/252 [09:01<09:25,  4.49s/it]\u001b[A\n",
            " 50% 127/252 [09:05<09:03,  4.34s/it]\u001b[A\n",
            " 51% 128/252 [09:09<08:46,  4.25s/it]\u001b[A\n",
            " 51% 129/252 [09:14<09:01,  4.41s/it]\u001b[A\n",
            " 52% 130/252 [09:18<08:44,  4.30s/it]\u001b[A\n",
            " 52% 131/252 [09:23<08:43,  4.33s/it]\u001b[A\n",
            " 52% 132/252 [09:27<08:55,  4.47s/it]\u001b[A\n",
            " 53% 133/252 [09:31<08:36,  4.34s/it]\u001b[A\n",
            " 53% 134/252 [09:35<08:23,  4.26s/it]\u001b[A\n",
            " 54% 135/252 [09:40<08:40,  4.45s/it]\u001b[A\n",
            " 54% 136/252 [09:45<08:34,  4.44s/it]\u001b[A\n",
            " 54% 137/252 [09:49<08:19,  4.34s/it]\u001b[A\n",
            " 55% 138/252 [09:54<08:28,  4.46s/it]\u001b[A\n",
            " 55% 139/252 [09:58<08:11,  4.35s/it]\u001b[A\n",
            " 56% 140/252 [10:02<07:56,  4.26s/it]\u001b[A\n",
            " 56% 141/252 [10:07<08:18,  4.49s/it]\u001b[A\n",
            " 56% 142/252 [10:11<07:58,  4.35s/it]\u001b[A\n",
            " 57% 143/252 [10:15<07:45,  4.27s/it]\u001b[A\n",
            " 57% 144/252 [10:20<07:55,  4.41s/it]\u001b[A\n",
            " 58% 145/252 [10:24<07:38,  4.29s/it]\u001b[A\n",
            " 58% 146/252 [10:28<07:38,  4.32s/it]\u001b[A\n",
            " 58% 147/252 [10:33<07:47,  4.45s/it]\u001b[A\n",
            " 59% 148/252 [10:37<07:29,  4.32s/it]\u001b[A\n",
            " 59% 149/252 [10:41<07:14,  4.22s/it]\u001b[A\n",
            " 60% 150/252 [10:46<07:26,  4.38s/it]\u001b[A\n",
            " 60% 151/252 [10:50<07:20,  4.37s/it]\u001b[A\n",
            " 60% 152/252 [10:54<07:05,  4.25s/it]\u001b[A\n",
            " 61% 153/252 [10:59<07:16,  4.41s/it]\u001b[A\n",
            " 61% 154/252 [11:03<07:00,  4.29s/it]\u001b[A\n",
            " 62% 155/252 [11:07<06:57,  4.31s/it]\u001b[A\n",
            " 62% 156/252 [11:12<07:03,  4.42s/it]\u001b[A\n",
            " 62% 157/252 [11:16<06:47,  4.29s/it]\u001b[A\n",
            " 63% 158/252 [11:20<06:35,  4.21s/it]\u001b[A\n",
            " 63% 159/252 [11:24<06:47,  4.38s/it]\u001b[A\n",
            " 63% 160/252 [11:29<06:42,  4.37s/it]\u001b[A\n",
            " 64% 161/252 [11:33<06:27,  4.26s/it]\u001b[A\n",
            " 64% 162/252 [11:37<06:34,  4.39s/it]\u001b[A\n",
            " 65% 163/252 [11:41<06:19,  4.27s/it]\u001b[A\n",
            " 65% 164/252 [11:45<06:08,  4.19s/it]\u001b[A\n",
            " 65% 165/252 [11:50<06:18,  4.35s/it]\u001b[A\n",
            " 66% 166/252 [11:55<06:13,  4.34s/it]\u001b[A\n",
            " 66% 167/252 [11:59<06:00,  4.24s/it]\u001b[A\n",
            " 67% 168/252 [12:03<06:08,  4.39s/it]\u001b[A\n",
            " 67% 169/252 [12:07<05:54,  4.27s/it]\u001b[A\n",
            " 67% 170/252 [12:11<05:43,  4.19s/it]\u001b[A\n",
            " 68% 171/252 [12:16<05:59,  4.44s/it]\u001b[A\n",
            " 68% 172/252 [12:20<05:45,  4.31s/it]\u001b[A\n",
            " 69% 173/252 [12:24<05:33,  4.22s/it]\u001b[A\n",
            " 69% 174/252 [12:29<05:41,  4.38s/it]\u001b[A\n",
            " 69% 175/252 [12:33<05:27,  4.26s/it]\u001b[A\n",
            " 70% 176/252 [12:37<05:24,  4.27s/it]\u001b[A\n",
            " 70% 177/252 [12:42<05:31,  4.41s/it]\u001b[A\n",
            " 71% 178/252 [12:46<05:17,  4.29s/it]\u001b[A\n",
            " 71% 179/252 [12:50<05:06,  4.20s/it]\u001b[A\n",
            " 71% 180/252 [12:55<05:13,  4.35s/it]\u001b[A\n",
            " 72% 181/252 [12:59<05:08,  4.35s/it]\u001b[A\n",
            " 72% 182/252 [13:03<04:56,  4.24s/it]\u001b[A\n",
            " 73% 183/252 [13:08<05:02,  4.38s/it]\u001b[A\n",
            " 73% 184/252 [13:12<04:49,  4.26s/it]\u001b[A\n",
            " 73% 185/252 [13:16<04:39,  4.18s/it]\u001b[A\n",
            " 74% 186/252 [13:21<04:52,  4.43s/it]\u001b[A\n",
            " 74% 187/252 [13:25<04:39,  4.30s/it]\u001b[A\n",
            " 75% 188/252 [13:29<04:29,  4.22s/it]\u001b[A\n",
            " 75% 189/252 [13:33<04:34,  4.35s/it]\u001b[A\n",
            " 75% 190/252 [13:37<04:23,  4.24s/it]\u001b[A\n",
            " 76% 191/252 [13:42<04:20,  4.27s/it]\u001b[A\n",
            " 76% 192/252 [13:46<04:23,  4.39s/it]\u001b[A\n",
            " 77% 193/252 [13:50<04:12,  4.27s/it]\u001b[A\n",
            " 77% 194/252 [13:54<04:02,  4.19s/it]\u001b[A\n",
            " 77% 195/252 [13:59<04:07,  4.34s/it]\u001b[A\n",
            " 78% 196/252 [14:03<04:02,  4.33s/it]\u001b[A\n",
            " 78% 197/252 [14:07<03:52,  4.23s/it]\u001b[A\n",
            " 79% 198/252 [14:12<03:55,  4.36s/it]\u001b[A\n",
            " 79% 199/252 [14:16<03:45,  4.25s/it]\u001b[A\n",
            " 79% 200/252 [14:20<03:37,  4.18s/it]\u001b[A\n",
            " 80% 201/252 [14:25<03:44,  4.41s/it]\u001b[A\n",
            " 80% 202/252 [14:29<03:34,  4.28s/it]\u001b[A\n",
            " 81% 203/252 [14:33<03:25,  4.19s/it]\u001b[A\n",
            " 81% 204/252 [14:38<03:27,  4.32s/it]\u001b[A\n",
            " 81% 205/252 [14:42<03:18,  4.23s/it]\u001b[A\n",
            " 82% 206/252 [14:46<03:15,  4.25s/it]\u001b[A\n",
            " 82% 207/252 [14:51<03:15,  4.35s/it]\u001b[A\n",
            " 83% 208/252 [14:55<03:07,  4.26s/it]\u001b[A\n",
            " 83% 209/252 [14:59<03:00,  4.20s/it]\u001b[A\n",
            " 83% 210/252 [15:03<03:00,  4.30s/it]\u001b[A\n",
            " 84% 211/252 [15:08<02:56,  4.32s/it]\u001b[A\n",
            " 84% 212/252 [15:12<02:48,  4.22s/it]\u001b[A\n",
            " 85% 213/252 [15:16<02:46,  4.28s/it]\u001b[A\n",
            " 85% 214/252 [15:20<02:40,  4.23s/it]\u001b[A\n",
            " 85% 215/252 [15:24<02:33,  4.16s/it]\u001b[A\n",
            " 86% 216/252 [15:29<02:35,  4.32s/it]\u001b[A\n",
            " 86% 217/252 [15:33<02:29,  4.28s/it]\u001b[A\n",
            " 87% 218/252 [15:37<02:22,  4.20s/it]\u001b[A\n",
            " 87% 219/252 [15:41<02:19,  4.22s/it]\u001b[A\n",
            " 87% 220/252 [15:46<02:18,  4.32s/it]\u001b[A\n",
            " 88% 221/252 [15:50<02:10,  4.22s/it]\u001b[A\n",
            " 88% 222/252 [15:54<02:06,  4.23s/it]\u001b[A\n",
            " 88% 223/252 [15:58<02:03,  4.27s/it]\u001b[A\n",
            " 89% 224/252 [16:02<01:57,  4.19s/it]\u001b[A\n",
            " 89% 225/252 [16:07<01:52,  4.19s/it]\u001b[A\n",
            " 90% 226/252 [16:11<01:53,  4.35s/it]\u001b[A\n",
            " 90% 227/252 [16:15<01:46,  4.24s/it]\u001b[A\n",
            " 90% 228/252 [16:19<01:41,  4.23s/it]\u001b[A\n",
            " 91% 229/252 [16:24<01:38,  4.29s/it]\u001b[A\n",
            " 91% 230/252 [16:28<01:32,  4.20s/it]\u001b[A\n",
            " 92% 231/252 [16:32<01:30,  4.29s/it]\u001b[A\n",
            " 92% 232/252 [16:37<01:26,  4.34s/it]\u001b[A\n",
            " 92% 233/252 [16:41<01:20,  4.23s/it]\u001b[A\n",
            " 93% 234/252 [16:45<01:15,  4.19s/it]\u001b[A\n",
            " 93% 235/252 [16:49<01:13,  4.30s/it]\u001b[A\n",
            " 94% 236/252 [16:53<01:07,  4.20s/it]\u001b[A\n",
            " 94% 237/252 [16:58<01:04,  4.27s/it]\u001b[A\n",
            " 94% 238/252 [17:02<01:01,  4.37s/it]\u001b[A\n",
            " 95% 239/252 [17:06<00:55,  4.26s/it]\u001b[A\n",
            " 95% 240/252 [17:11<00:50,  4.19s/it]\u001b[A\n",
            " 96% 241/252 [17:15<00:47,  4.32s/it]\u001b[A\n",
            " 96% 242/252 [17:19<00:43,  4.32s/it]\u001b[A\n",
            " 96% 243/252 [17:23<00:38,  4.23s/it]\u001b[A\n",
            " 97% 244/252 [17:28<00:34,  4.35s/it]\u001b[A\n",
            " 97% 245/252 [17:32<00:29,  4.25s/it]\u001b[A\n",
            " 98% 246/252 [17:36<00:25,  4.17s/it]\u001b[A\n",
            " 98% 247/252 [17:41<00:22,  4.44s/it]\u001b[A\n",
            " 98% 248/252 [17:45<00:17,  4.31s/it]\u001b[A\n",
            " 99% 249/252 [17:49<00:12,  4.22s/it]\u001b[A\n",
            " 99% 250/252 [17:54<00:08,  4.37s/it]\u001b[A\n",
            "100% 251/252 [17:58<00:04,  4.26s/it]\u001b[A\n",
            "100% 252/252 [18:02<00:00,  4.15s/it]\u001b[AINFO:absl:Using default tokenizer.\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.02852761745452881, 'eval_rouge1': 27.8099, 'eval_rouge2': 22.4637, 'eval_rougeL': 27.7612, 'eval_rougeLsum': 27.7762, 'eval_runtime': 1089.249, 'eval_samples_per_second': 1.849, 'eval_steps_per_second': 0.231, 'epoch': 0.13}\n",
            "  4% 300/6798 [58:25<43:11,  2.51it/s]\n",
            "100% 252/252 [18:04<00:00,  4.15s/it]\u001b[A\n",
            "{'loss': 0.0037, 'grad_norm': 0.05119054764509201, 'learning_rate': 1.8837893498087674e-05, 'epoch': 0.18}\n",
            "  6% 400/6798 [59:53<40:58,  2.60it/s]\n",
            "  0% 0/252 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/252 [00:05<10:52,  2.61s/it]\u001b[A\n",
            "  1% 3/252 [00:10<16:16,  3.92s/it]\u001b[A\n",
            "  2% 4/252 [00:16<18:04,  4.37s/it]\u001b[A\n",
            "  2% 5/252 [00:20<17:58,  4.37s/it]\u001b[A\n",
            "  2% 6/252 [00:24<17:54,  4.37s/it]\u001b[A\n",
            "  3% 7/252 [00:29<17:40,  4.33s/it]\u001b[A\n",
            "  3% 8/252 [00:33<17:10,  4.23s/it]\u001b[A\n",
            "  4% 9/252 [00:37<17:37,  4.35s/it]\u001b[A\n",
            "  4% 10/252 [00:41<17:25,  4.32s/it]\u001b[A\n",
            "  4% 11/252 [00:45<16:57,  4.22s/it]\u001b[A\n",
            "  5% 12/252 [00:50<16:52,  4.22s/it]\u001b[A\n",
            "  5% 13/252 [00:54<17:01,  4.27s/it]\u001b[A\n",
            "  6% 14/252 [00:58<16:58,  4.28s/it]\u001b[A\n",
            "  6% 15/252 [01:03<16:48,  4.25s/it]\u001b[A\n",
            "  6% 16/252 [01:07<16:53,  4.30s/it]\u001b[A\n",
            "  7% 17/252 [01:11<16:27,  4.20s/it]\u001b[A\n",
            "  7% 18/252 [01:15<16:15,  4.17s/it]\u001b[A\n",
            "  8% 19/252 [01:20<16:59,  4.37s/it]\u001b[A\n",
            "  8% 20/252 [01:24<16:29,  4.27s/it]\u001b[A\n",
            "  8% 21/252 [01:28<16:12,  4.21s/it]\u001b[A\n",
            "  9% 22/252 [01:33<16:32,  4.31s/it]\u001b[A\n",
            "  9% 23/252 [01:37<16:28,  4.32s/it]\u001b[A\n",
            " 10% 24/252 [01:41<16:06,  4.24s/it]\u001b[A\n",
            " 10% 25/252 [01:45<16:25,  4.34s/it]\u001b[A\n",
            " 10% 26/252 [01:50<16:00,  4.25s/it]\u001b[A\n",
            " 11% 27/252 [01:54<15:41,  4.18s/it]\u001b[A\n",
            " 11% 28/252 [01:58<16:25,  4.40s/it]\u001b[A\n",
            " 12% 29/252 [02:02<15:53,  4.27s/it]\u001b[A\n",
            " 12% 30/252 [02:06<15:32,  4.20s/it]\u001b[A\n",
            " 12% 31/252 [02:11<15:56,  4.33s/it]\u001b[A\n",
            " 13% 32/252 [02:15<15:49,  4.32s/it]\u001b[A\n",
            " 13% 33/252 [02:19<15:29,  4.25s/it]\u001b[A\n",
            " 13% 34/252 [02:24<15:53,  4.37s/it]\u001b[A\n",
            " 14% 35/252 [02:28<15:23,  4.26s/it]\u001b[A\n",
            " 14% 36/252 [02:32<15:24,  4.28s/it]\u001b[A\n",
            " 15% 37/252 [02:37<15:44,  4.40s/it]\u001b[A\n",
            " 15% 38/252 [02:41<15:14,  4.27s/it]\u001b[A\n",
            " 15% 39/252 [02:45<14:54,  4.20s/it]\u001b[A\n",
            " 16% 40/252 [02:50<15:42,  4.45s/it]\u001b[A\n",
            " 16% 41/252 [02:54<15:10,  4.31s/it]\u001b[A\n",
            " 17% 42/252 [02:58<14:44,  4.21s/it]\u001b[A\n",
            " 17% 43/252 [03:03<15:09,  4.35s/it]\u001b[A\n",
            " 17% 44/252 [03:07<14:42,  4.24s/it]\u001b[A\n",
            " 18% 45/252 [03:11<14:41,  4.26s/it]\u001b[A\n",
            " 18% 46/252 [03:16<15:05,  4.40s/it]\u001b[A\n",
            " 19% 47/252 [03:20<14:37,  4.28s/it]\u001b[A\n",
            " 19% 48/252 [03:24<14:17,  4.21s/it]\u001b[A\n",
            " 19% 49/252 [03:29<14:46,  4.37s/it]\u001b[A\n",
            " 20% 50/252 [03:33<14:39,  4.35s/it]\u001b[A\n",
            " 20% 51/252 [03:37<14:12,  4.24s/it]\u001b[A\n",
            " 21% 52/252 [03:42<14:36,  4.38s/it]\u001b[A\n",
            " 21% 53/252 [03:46<14:08,  4.26s/it]\u001b[A\n",
            " 21% 54/252 [03:50<14:07,  4.28s/it]\u001b[A\n",
            " 22% 55/252 [03:55<14:30,  4.42s/it]\u001b[A\n",
            " 22% 56/252 [03:59<14:01,  4.29s/it]\u001b[A\n",
            " 23% 57/252 [04:03<13:40,  4.21s/it]\u001b[A\n",
            " 23% 58/252 [04:07<14:06,  4.36s/it]\u001b[A\n",
            " 23% 59/252 [04:12<13:56,  4.34s/it]\u001b[A\n",
            " 24% 60/252 [04:16<13:31,  4.23s/it]\u001b[A\n",
            " 24% 61/252 [04:20<13:55,  4.38s/it]\u001b[A\n",
            " 25% 62/252 [04:24<13:32,  4.28s/it]\u001b[A\n",
            " 25% 63/252 [04:28<13:13,  4.20s/it]\u001b[A\n",
            " 25% 64/252 [04:34<13:59,  4.47s/it]\u001b[A\n",
            " 26% 65/252 [04:38<13:30,  4.33s/it]\u001b[A\n",
            " 26% 66/252 [04:42<13:07,  4.23s/it]\u001b[A\n",
            " 27% 67/252 [04:46<13:32,  4.39s/it]\u001b[A\n",
            " 27% 68/252 [04:51<13:25,  4.38s/it]\u001b[A\n",
            " 27% 69/252 [04:55<13:03,  4.28s/it]\u001b[A\n",
            " 28% 70/252 [04:59<13:25,  4.42s/it]\u001b[A\n",
            " 28% 71/252 [05:03<12:58,  4.30s/it]\u001b[A\n",
            " 29% 72/252 [05:07<12:38,  4.21s/it]\u001b[A\n",
            " 29% 73/252 [05:13<13:18,  4.46s/it]\u001b[A\n",
            " 29% 74/252 [05:17<12:50,  4.33s/it]\u001b[A\n",
            " 30% 75/252 [05:21<12:31,  4.25s/it]\u001b[A\n",
            " 30% 76/252 [05:25<12:57,  4.42s/it]\u001b[A\n",
            " 31% 77/252 [05:30<12:45,  4.37s/it]\u001b[A\n",
            " 31% 78/252 [05:34<12:19,  4.25s/it]\u001b[A\n",
            " 31% 79/252 [05:38<12:42,  4.40s/it]\u001b[A\n",
            " 32% 80/252 [05:42<12:14,  4.27s/it]\u001b[A\n",
            " 32% 81/252 [05:46<11:55,  4.19s/it]\u001b[A\n",
            " 33% 82/252 [05:51<12:30,  4.41s/it]\u001b[A\n",
            " 33% 83/252 [05:55<12:06,  4.30s/it]\u001b[A\n",
            " 33% 84/252 [05:59<11:45,  4.20s/it]\u001b[A\n",
            " 34% 85/252 [06:04<12:05,  4.35s/it]\u001b[A\n",
            " 34% 86/252 [06:08<11:58,  4.33s/it]\u001b[A\n",
            " 35% 87/252 [06:12<11:38,  4.24s/it]\u001b[A\n",
            " 35% 88/252 [06:17<11:58,  4.38s/it]\u001b[A\n",
            " 35% 89/252 [06:21<11:34,  4.26s/it]\u001b[A\n",
            " 36% 90/252 [06:25<11:17,  4.18s/it]\u001b[A\n",
            " 36% 91/252 [06:30<11:47,  4.39s/it]\u001b[A\n",
            " 37% 92/252 [06:34<11:24,  4.28s/it]\u001b[A\n",
            " 37% 93/252 [06:38<11:06,  4.19s/it]\u001b[A\n",
            " 37% 94/252 [06:43<11:22,  4.32s/it]\u001b[A\n",
            " 38% 95/252 [06:47<11:18,  4.32s/it]\u001b[A\n",
            " 38% 96/252 [06:51<10:58,  4.22s/it]\u001b[A\n",
            " 38% 97/252 [06:55<11:11,  4.33s/it]\u001b[A\n",
            " 39% 98/252 [06:59<10:51,  4.23s/it]\u001b[A\n",
            " 39% 99/252 [07:03<10:35,  4.15s/it]\u001b[A\n",
            " 40% 100/252 [07:08<11:04,  4.37s/it]\u001b[A\n",
            " 40% 101/252 [07:12<10:42,  4.25s/it]\u001b[A\n",
            " 40% 102/252 [07:16<10:26,  4.18s/it]\u001b[A\n",
            " 41% 103/252 [07:21<10:32,  4.24s/it]\u001b[A\n",
            " 41% 104/252 [07:25<10:37,  4.30s/it]\u001b[A\n",
            " 42% 105/252 [07:29<10:17,  4.20s/it]\u001b[A\n",
            " 42% 106/252 [07:33<10:20,  4.25s/it]\u001b[A\n",
            " 42% 107/252 [07:38<10:13,  4.23s/it]\u001b[A\n",
            " 43% 108/252 [07:42<09:58,  4.15s/it]\u001b[A\n",
            " 43% 109/252 [07:46<10:15,  4.30s/it]\u001b[A\n",
            " 44% 110/252 [07:50<10:07,  4.28s/it]\u001b[A\n",
            " 44% 111/252 [07:54<09:51,  4.19s/it]\u001b[A\n",
            " 44% 112/252 [07:59<09:48,  4.20s/it]\u001b[A\n",
            " 45% 113/252 [08:03<10:02,  4.33s/it]\u001b[A\n",
            " 45% 114/252 [08:07<09:43,  4.23s/it]\u001b[A\n",
            " 46% 115/252 [08:11<09:37,  4.22s/it]\u001b[A\n",
            " 46% 116/252 [08:16<09:39,  4.26s/it]\u001b[A\n",
            " 46% 117/252 [08:20<09:25,  4.19s/it]\u001b[A\n",
            " 47% 118/252 [08:24<09:20,  4.19s/it]\u001b[A\n",
            " 47% 119/252 [08:29<09:42,  4.38s/it]\u001b[A\n",
            " 48% 120/252 [08:33<09:24,  4.28s/it]\u001b[A\n",
            " 48% 121/252 [08:37<09:16,  4.25s/it]\u001b[A\n",
            " 48% 122/252 [08:42<09:23,  4.34s/it]\u001b[A\n",
            " 49% 123/252 [08:46<09:19,  4.34s/it]\u001b[A\n",
            " 49% 124/252 [08:50<09:08,  4.29s/it]\u001b[A\n",
            " 50% 125/252 [08:55<09:12,  4.35s/it]\u001b[A\n",
            " 50% 126/252 [08:59<08:55,  4.25s/it]\u001b[A\n",
            " 50% 127/252 [09:03<09:01,  4.33s/it]\u001b[A\n",
            " 51% 128/252 [09:08<09:01,  4.37s/it]\u001b[A\n",
            " 51% 129/252 [09:12<08:43,  4.26s/it]\u001b[A\n",
            " 52% 130/252 [09:16<08:35,  4.22s/it]\u001b[A\n",
            " 52% 131/252 [09:20<08:45,  4.35s/it]\u001b[A\n",
            " 52% 132/252 [09:25<08:41,  4.35s/it]\u001b[A\n",
            " 53% 133/252 [09:29<08:30,  4.29s/it]\u001b[A\n",
            " 53% 134/252 [09:33<08:35,  4.37s/it]\u001b[A\n",
            " 54% 135/252 [09:37<08:18,  4.26s/it]\u001b[A\n",
            " 54% 136/252 [09:42<08:08,  4.21s/it]\u001b[A\n",
            " 54% 137/252 [09:47<08:29,  4.43s/it]\u001b[A\n",
            " 55% 138/252 [09:51<08:11,  4.31s/it]\u001b[A\n",
            " 55% 139/252 [09:55<08:02,  4.27s/it]\u001b[A\n",
            " 56% 140/252 [09:59<08:07,  4.35s/it]\u001b[A\n",
            " 56% 141/252 [10:04<08:00,  4.33s/it]\u001b[A\n",
            " 56% 142/252 [10:08<07:47,  4.25s/it]\u001b[A\n",
            " 57% 143/252 [10:12<07:53,  4.34s/it]\u001b[A\n",
            " 57% 144/252 [10:16<07:37,  4.24s/it]\u001b[A\n",
            " 58% 145/252 [10:21<07:38,  4.28s/it]\u001b[A\n",
            " 58% 146/252 [10:25<07:42,  4.37s/it]\u001b[A\n",
            " 58% 147/252 [10:29<07:26,  4.25s/it]\u001b[A\n",
            " 59% 148/252 [10:33<07:13,  4.16s/it]\u001b[A\n",
            " 59% 149/252 [10:38<07:24,  4.31s/it]\u001b[A\n",
            " 60% 150/252 [10:42<07:19,  4.31s/it]\u001b[A\n",
            " 60% 151/252 [10:46<07:06,  4.22s/it]\u001b[A\n",
            " 60% 152/252 [10:51<07:16,  4.36s/it]\u001b[A\n",
            " 61% 153/252 [10:55<07:01,  4.26s/it]\u001b[A\n",
            " 61% 154/252 [10:59<06:58,  4.27s/it]\u001b[A\n",
            " 62% 155/252 [11:04<07:05,  4.39s/it]\u001b[A\n",
            " 62% 156/252 [11:08<06:49,  4.27s/it]\u001b[A\n",
            " 62% 157/252 [11:12<06:36,  4.18s/it]\u001b[A\n",
            " 63% 158/252 [11:16<06:47,  4.33s/it]\u001b[A\n",
            " 63% 159/252 [11:21<06:41,  4.32s/it]\u001b[A\n",
            " 63% 160/252 [11:25<06:28,  4.22s/it]\u001b[A\n",
            " 64% 161/252 [11:29<06:36,  4.36s/it]\u001b[A\n",
            " 64% 162/252 [11:33<06:22,  4.25s/it]\u001b[A\n",
            " 65% 163/252 [11:38<06:19,  4.26s/it]\u001b[A\n",
            " 65% 164/252 [11:42<06:27,  4.40s/it]\u001b[A\n",
            " 65% 165/252 [11:46<06:11,  4.27s/it]\u001b[A\n",
            " 66% 166/252 [11:50<05:59,  4.18s/it]\u001b[A\n",
            " 66% 167/252 [11:55<06:07,  4.32s/it]\u001b[A\n",
            " 67% 168/252 [11:59<06:02,  4.31s/it]\u001b[A\n",
            " 67% 169/252 [12:03<05:49,  4.22s/it]\u001b[A\n",
            " 67% 170/252 [12:08<05:56,  4.34s/it]\u001b[A\n",
            " 68% 171/252 [12:12<05:42,  4.23s/it]\u001b[A\n",
            " 68% 172/252 [12:16<05:33,  4.16s/it]\u001b[A\n",
            " 69% 173/252 [12:21<05:48,  4.41s/it]\u001b[A\n",
            " 69% 174/252 [12:25<05:33,  4.28s/it]\u001b[A\n",
            " 69% 175/252 [12:29<05:22,  4.19s/it]\u001b[A\n",
            " 70% 176/252 [12:33<05:28,  4.32s/it]\u001b[A\n",
            " 70% 177/252 [12:37<05:15,  4.21s/it]\u001b[A\n",
            " 71% 178/252 [12:42<05:12,  4.23s/it]\u001b[A\n",
            " 71% 179/252 [12:46<05:17,  4.35s/it]\u001b[A\n",
            " 71% 180/252 [12:50<05:05,  4.24s/it]\u001b[A\n",
            " 72% 181/252 [12:54<04:56,  4.17s/it]\u001b[A\n",
            " 72% 182/252 [12:59<05:06,  4.38s/it]\u001b[A\n",
            " 73% 183/252 [13:03<04:52,  4.25s/it]\u001b[A\n",
            " 73% 184/252 [13:07<04:43,  4.16s/it]\u001b[A\n",
            " 73% 185/252 [13:11<04:44,  4.25s/it]\u001b[A\n",
            " 74% 186/252 [13:15<04:35,  4.17s/it]\u001b[A\n",
            " 74% 187/252 [13:20<04:33,  4.21s/it]\u001b[A\n",
            " 75% 188/252 [13:24<04:33,  4.28s/it]\u001b[A\n",
            " 75% 189/252 [13:28<04:25,  4.21s/it]\u001b[A\n",
            " 75% 190/252 [13:32<04:16,  4.14s/it]\u001b[A\n",
            " 76% 191/252 [13:36<04:15,  4.18s/it]\u001b[A\n",
            " 76% 192/252 [13:41<04:18,  4.31s/it]\u001b[A\n",
            " 77% 193/252 [13:45<04:08,  4.22s/it]\u001b[A\n",
            " 77% 194/252 [13:49<04:05,  4.24s/it]\u001b[A\n",
            " 77% 195/252 [13:54<04:02,  4.25s/it]\u001b[A\n",
            " 78% 196/252 [13:58<03:59,  4.28s/it]\u001b[A\n",
            " 78% 197/252 [14:02<03:54,  4.26s/it]\u001b[A\n",
            " 79% 198/252 [14:06<03:50,  4.27s/it]\u001b[A\n",
            " 79% 199/252 [14:10<03:41,  4.19s/it]\u001b[A\n",
            " 79% 200/252 [14:15<03:37,  4.18s/it]\u001b[A\n",
            " 80% 201/252 [14:19<03:42,  4.37s/it]\u001b[A\n",
            " 80% 202/252 [14:23<03:32,  4.25s/it]\u001b[A\n",
            " 81% 203/252 [14:28<03:26,  4.22s/it]\u001b[A\n",
            " 81% 204/252 [14:32<03:26,  4.29s/it]\u001b[A\n",
            " 81% 205/252 [14:36<03:21,  4.29s/it]\u001b[A\n",
            " 82% 206/252 [14:40<03:14,  4.23s/it]\u001b[A\n",
            " 82% 207/252 [14:45<03:14,  4.31s/it]\u001b[A\n",
            " 83% 208/252 [14:49<03:05,  4.22s/it]\u001b[A\n",
            " 83% 209/252 [14:53<02:59,  4.17s/it]\u001b[A\n",
            " 83% 210/252 [14:58<03:04,  4.39s/it]\u001b[A\n",
            " 84% 211/252 [15:02<02:54,  4.26s/it]\u001b[A\n",
            " 84% 212/252 [15:06<02:47,  4.19s/it]\u001b[A\n",
            " 85% 213/252 [15:10<02:47,  4.30s/it]\u001b[A\n",
            " 85% 214/252 [15:14<02:39,  4.20s/it]\u001b[A\n",
            " 85% 215/252 [15:19<02:37,  4.25s/it]\u001b[A\n",
            " 86% 216/252 [15:23<02:36,  4.35s/it]\u001b[A\n",
            " 86% 217/252 [15:27<02:28,  4.25s/it]\u001b[A\n",
            " 87% 218/252 [15:31<02:21,  4.16s/it]\u001b[A\n",
            " 87% 219/252 [15:36<02:25,  4.42s/it]\u001b[A\n",
            " 87% 220/252 [15:40<02:17,  4.29s/it]\u001b[A\n",
            " 88% 221/252 [15:44<02:10,  4.19s/it]\u001b[A\n",
            " 88% 222/252 [15:49<02:10,  4.35s/it]\u001b[A\n",
            " 88% 223/252 [15:53<02:02,  4.24s/it]\u001b[A\n",
            " 89% 224/252 [15:57<02:00,  4.32s/it]\u001b[A\n",
            " 89% 225/252 [16:02<01:59,  4.42s/it]\u001b[A\n",
            " 90% 226/252 [16:06<01:51,  4.30s/it]\u001b[A\n",
            " 90% 227/252 [16:10<01:44,  4.20s/it]\u001b[A\n",
            " 90% 228/252 [16:15<01:46,  4.43s/it]\u001b[A\n",
            " 91% 229/252 [16:19<01:38,  4.30s/it]\u001b[A\n",
            " 91% 230/252 [16:23<01:32,  4.20s/it]\u001b[A\n",
            " 92% 231/252 [16:28<01:31,  4.36s/it]\u001b[A\n",
            " 92% 232/252 [16:32<01:25,  4.25s/it]\u001b[A\n",
            " 92% 233/252 [16:36<01:21,  4.27s/it]\u001b[A\n",
            " 93% 234/252 [16:41<01:19,  4.41s/it]\u001b[A\n",
            " 93% 235/252 [16:45<01:12,  4.29s/it]\u001b[A\n",
            " 94% 236/252 [16:49<01:07,  4.20s/it]\u001b[A\n",
            " 94% 237/252 [16:54<01:05,  4.35s/it]\u001b[A\n",
            " 94% 238/252 [16:58<01:00,  4.34s/it]\u001b[A\n",
            " 95% 239/252 [17:02<00:55,  4.23s/it]\u001b[A\n",
            " 95% 240/252 [17:07<00:52,  4.37s/it]\u001b[A\n",
            " 96% 241/252 [17:10<00:46,  4.25s/it]\u001b[A\n",
            " 96% 242/252 [17:15<00:42,  4.27s/it]\u001b[A\n",
            " 96% 243/252 [17:19<00:39,  4.39s/it]\u001b[A\n",
            " 97% 244/252 [17:23<00:34,  4.27s/it]\u001b[A\n",
            " 97% 245/252 [17:27<00:29,  4.19s/it]\u001b[A\n",
            " 98% 246/252 [17:32<00:26,  4.41s/it]\u001b[A\n",
            " 98% 247/252 [17:36<00:21,  4.27s/it]\u001b[A\n",
            " 98% 248/252 [17:40<00:16,  4.19s/it]\u001b[A\n",
            " 99% 249/252 [17:45<00:13,  4.34s/it]\u001b[A\n",
            " 99% 250/252 [17:49<00:08,  4.24s/it]\u001b[A\n",
            "100% 251/252 [17:53<00:04,  4.25s/it]\u001b[A\n",
            "100% 252/252 [17:58<00:00,  4.29s/it]\u001b[AINFO:absl:Using default tokenizer.\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.02145739272236824, 'eval_rouge1': 24.3441, 'eval_rouge2': 19.6224, 'eval_rougeL': 24.2973, 'eval_rougeLsum': 24.3108, 'eval_runtime': 1086.1606, 'eval_samples_per_second': 1.854, 'eval_steps_per_second': 0.232, 'epoch': 0.18}\n",
            "  6% 400/6798 [1:17:59<40:58,  2.60it/s]\n",
            "100% 252/252 [17:59<00:00,  4.29s/it]\u001b[A\n",
            "{'loss': 0.0022, 'grad_norm': 1.30003023147583, 'learning_rate': 1.8543689320388352e-05, 'epoch': 0.22}\n",
            "  7% 500/6798 [1:19:11<40:10,  2.61it/s]\n",
            "  0% 0/252 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/252 [00:05<11:10,  2.68s/it]\u001b[A\n",
            "  1% 3/252 [00:10<16:09,  3.89s/it]\u001b[A\n",
            "  2% 4/252 [00:15<17:38,  4.27s/it]\u001b[A\n",
            "  2% 5/252 [00:19<17:11,  4.18s/it]\u001b[A\n",
            "  2% 6/252 [00:23<17:03,  4.16s/it]\u001b[A\n",
            "  3% 7/252 [00:28<17:29,  4.29s/it]\u001b[A\n",
            "  3% 8/252 [00:32<17:30,  4.30s/it]\u001b[A\n",
            "  4% 9/252 [00:36<17:07,  4.23s/it]\u001b[A\n",
            "  4% 10/252 [00:41<17:31,  4.34s/it]\u001b[A\n",
            "  4% 11/252 [00:45<16:59,  4.23s/it]\u001b[A\n",
            "  5% 12/252 [00:49<16:40,  4.17s/it]\u001b[A\n",
            "  5% 13/252 [00:54<17:34,  4.41s/it]\u001b[A\n",
            "  6% 14/252 [00:58<17:01,  4.29s/it]\u001b[A\n",
            "  6% 15/252 [01:02<16:36,  4.21s/it]\u001b[A\n",
            "  6% 16/252 [01:07<17:06,  4.35s/it]\u001b[A\n",
            "  7% 17/252 [01:11<17:02,  4.35s/it]\u001b[A\n",
            "  7% 18/252 [01:15<16:32,  4.24s/it]\u001b[A\n",
            "  8% 19/252 [01:20<16:58,  4.37s/it]\u001b[A\n",
            "  8% 20/252 [01:24<16:29,  4.27s/it]\u001b[A\n",
            "  8% 21/252 [01:28<16:33,  4.30s/it]\u001b[A\n",
            "  9% 22/252 [01:33<16:51,  4.40s/it]\u001b[A\n",
            "  9% 23/252 [01:37<16:19,  4.28s/it]\u001b[A\n",
            " 10% 24/252 [01:41<16:03,  4.23s/it]\u001b[A\n",
            " 10% 25/252 [01:46<16:36,  4.39s/it]\u001b[A\n",
            " 10% 26/252 [01:50<16:31,  4.38s/it]\u001b[A\n",
            " 11% 27/252 [01:54<16:10,  4.31s/it]\u001b[A\n",
            " 11% 28/252 [01:59<16:36,  4.45s/it]\u001b[A\n",
            " 12% 29/252 [02:03<16:05,  4.33s/it]\u001b[A\n",
            " 12% 30/252 [02:07<15:38,  4.23s/it]\u001b[A\n",
            " 12% 31/252 [02:12<16:29,  4.48s/it]\u001b[A\n",
            " 13% 32/252 [02:16<15:54,  4.34s/it]\u001b[A\n",
            " 13% 33/252 [02:20<15:29,  4.24s/it]\u001b[A\n",
            " 13% 34/252 [02:25<15:55,  4.38s/it]\u001b[A\n",
            " 14% 35/252 [02:29<15:47,  4.37s/it]\u001b[A\n",
            " 14% 36/252 [02:33<15:18,  4.25s/it]\u001b[A\n",
            " 15% 37/252 [02:38<15:42,  4.39s/it]\u001b[A\n",
            " 15% 38/252 [02:42<15:14,  4.27s/it]\u001b[A\n",
            " 15% 39/252 [02:46<14:51,  4.19s/it]\u001b[A\n",
            " 16% 40/252 [02:51<15:40,  4.44s/it]\u001b[A\n",
            " 16% 41/252 [02:55<15:08,  4.31s/it]\u001b[A\n",
            " 17% 42/252 [02:59<14:46,  4.22s/it]\u001b[A\n",
            " 17% 43/252 [03:04<15:13,  4.37s/it]\u001b[A\n",
            " 17% 44/252 [03:08<15:05,  4.35s/it]\u001b[A\n",
            " 18% 45/252 [03:12<14:38,  4.24s/it]\u001b[A\n",
            " 18% 46/252 [03:17<15:03,  4.39s/it]\u001b[A\n",
            " 19% 47/252 [03:21<14:35,  4.27s/it]\u001b[A\n",
            " 19% 48/252 [03:25<14:15,  4.19s/it]\u001b[A\n",
            " 19% 49/252 [03:30<15:01,  4.44s/it]\u001b[A\n",
            " 20% 50/252 [03:34<14:30,  4.31s/it]\u001b[A\n",
            " 20% 51/252 [03:38<14:07,  4.21s/it]\u001b[A\n",
            " 21% 52/252 [03:42<14:34,  4.37s/it]\u001b[A\n",
            " 21% 53/252 [03:47<14:26,  4.35s/it]\u001b[A\n",
            " 21% 54/252 [03:51<13:59,  4.24s/it]\u001b[A\n",
            " 22% 55/252 [03:55<14:23,  4.38s/it]\u001b[A\n",
            " 22% 56/252 [03:59<13:56,  4.27s/it]\u001b[A\n",
            " 23% 57/252 [04:03<13:37,  4.19s/it]\u001b[A\n",
            " 23% 58/252 [04:08<14:14,  4.40s/it]\u001b[A\n",
            " 23% 59/252 [04:12<13:48,  4.29s/it]\u001b[A\n",
            " 24% 60/252 [04:16<13:26,  4.20s/it]\u001b[A\n",
            " 24% 61/252 [04:21<13:55,  4.37s/it]\u001b[ATraceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/cah-app/python-backend/src/model_comparison.py\", line 234, in <module>\n",
            "  File \"/content/drive/MyDrive/cah-app/python-backend/src/model_comparison.py\", line 216, in main\n",
            "    trainer.train(resume_from_checkpoint=last_checkpoint)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 2245, in train\n",
            "    return inner_training_loop(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 2627, in _inner_training_loop\n",
            "    self._maybe_log_save_evaluate(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 3096, in _maybe_log_save_evaluate\n",
            "    metrics = self._evaluate(trial, ignore_keys_for_eval)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 3045, in _evaluate\n",
            "    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer_seq2seq.py\", line 197, in evaluate\n",
            "    return super().evaluate(eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 4154, in evaluate\n",
            "    output = eval_loop(\n",
            "             ^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 4348, in evaluation_loop\n",
            "    losses, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n",
            "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer_seq2seq.py\", line 333, in prediction_step\n",
            "    generated_tokens = self.model.generate(**generation_inputs, **gen_kwargs)\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\", line 2484, in generate\n",
            "    result = self._beam_search(\n",
            "             ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\", line 3904, in _beam_search\n",
            "    model_outputs = self(**model_inputs, return_dict=True)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\", line 814, in forward\n",
            "    return model_forward(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\", line 802, in __call__\n",
            "    return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/amp/autocast_mode.py\", line 44, in decorate_autocast\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bart/modeling_bart.py\", line 1654, in forward\n",
            "    outputs = self.model(\n",
            "              ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bart/modeling_bart.py\", line 1533, in forward\n",
            "    decoder_outputs = self.decoder(\n",
            "                      ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bart/modeling_bart.py\", line 1378, in forward\n",
            "    layer_outputs = decoder_layer(\n",
            "                    ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/bart/modeling_bart.py\", line 693, in forward\n",
            "    hidden_states = self.encoder_attn_layer_norm(hidden_states)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/normalization.py\", line 217, in forward\n",
            "    return F.layer_norm(\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\", line 2910, in layer_norm\n",
            "    return torch.layer_norm(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "  7%|▋         | 500/6798 [1:23:41<17:34:11, 10.04s/it]\n",
            "\n",
            "                                    \u001b[A^C\n"
          ]
        }
      ],
      "source": [
        "!python src/model_comparison.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: True\n",
            "Current device: 0\n",
            "Device name: NVIDIA A30\n",
            "Allocated memory (MB): 0.0\n",
            "Reserved memory (MB): 0.0\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers>=4.39.0\n",
            "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: filelock in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from transformers>=4.39.0) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from transformers>=4.39.0) (0.31.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from transformers>=4.39.0) (2.1.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from transformers>=4.39.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from transformers>=4.39.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from transformers>=4.39.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from transformers>=4.39.0) (2.32.3)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers>=4.39.0)\n",
            "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from transformers>=4.39.0) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from transformers>=4.39.0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.39.0) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.39.0) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.39.0) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from requests->transformers>=4.39.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from requests->transformers>=4.39.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from requests->transformers>=4.39.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages (from requests->transformers>=4.39.0) (2025.4.26)\n",
            "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m97.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.2\n",
            "    Uninstalling tokenizers-0.15.2:\n",
            "      Successfully uninstalled tokenizers-0.15.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.38.1\n",
            "    Uninstalling transformers-4.38.1:\n",
            "      Successfully uninstalled transformers-4.38.1\n",
            "Successfully installed tokenizers-0.21.1 transformers-4.51.3\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade \"transformers>=4.39.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1093"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch, gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU available: True\n",
            "GPU device: NVIDIA A30\n",
            "Using project directory: /homes/ag724\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Verify GPU availability\n",
        "import torch\n",
        "print('GPU available:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU device:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# Set up project directory\n",
        "import os\n",
        "PROJECT_DIR = os.getcwd()\n",
        "print(f\"Using project directory: {PROJECT_DIR}\")\n",
        "\n",
        "# Create necessary directories if they don't exist\n",
        "!mkdir -p data/{raw,processed,debug}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thu May  8 16:17:14 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.120                Driver Version: 550.120        CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A30                     Off |   00000000:01:00.0 Off |                    0 |\n",
            "| N/A   29C    P0             27W /  165W |       4MiB /  24576MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU memory allocated: 0.00 GB\n",
            "GPU memory reserved: 0.00 GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "# Delete all variables that might hold references\n",
        "for obj in dir():\n",
        "    if obj not in ['torch', 'gc'] and not obj.startswith('__'):\n",
        "        del globals()[obj]\n",
        "\n",
        "# Force CUDA operations to complete\n",
        "torch.cuda.synchronize()\n",
        "\n",
        "# Clear cache and run garbage collection\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Check memory status\n",
        "print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
        "print(f\"GPU memory reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:__main__:Loading data...\n",
            "INFO:__main__:Using 2013 examples for quick model comparison\n",
            "INFO:__main__:\n",
            "Quick comparison training for BART...\n",
            "Map: 100%|█████████████████████████| 1811/1811 [00:00<00:00, 9476.59 examples/s]\n",
            "Map: 100%|███████████████████████████| 202/202 [00:00<00:00, 8503.45 examples/s]\n",
            "INFO:__main__:Found checkpoint: models/bart-large-cnn/checkpoint-300\n",
            "/vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages/transformers/models/bart/configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n",
            "  warnings.warn(\n",
            "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n",
            "Warning: The following arguments do not match the ones in the `trainer_state.json` within the checkpoint directory: \n",
            "\tlogging_steps: 50 (from args) != 100 (from trainer_state.json)\n",
            "\tper_device_train_batch_size: 16 (from args) != 8 (from trainer_state.json)\n",
            "  0%|                                                   | 0/113 [00:00<?, ?it/s]There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n",
            "{'train_runtime': 0.2914, 'train_samples_per_second': 6214.429, 'train_steps_per_second': 387.758, 'train_loss': 0.0, 'epoch': 0.13}\n",
            "  0%|                                                   | 0/113 [00:00<?, ?it/s]\n",
            "100%|███████████████████████████████████████████| 13/13 [00:41<00:00,  3.26s/it]INFO:absl:Using default tokenizer.\n",
            "100%|███████████████████████████████████████████| 13/13 [00:42<00:00,  3.29s/it]\n",
            "INFO:__main__:BART Results:\n",
            "INFO:__main__:eval_loss: 0.026430455967783928\n",
            "INFO:__main__:eval_rouge1: 29.4827\n",
            "INFO:__main__:eval_rouge2: 23.695\n",
            "INFO:__main__:eval_rougeL: 29.5668\n",
            "INFO:__main__:eval_rougeLsum: 29.482\n",
            "INFO:__main__:eval_runtime: 46.7493\n",
            "INFO:__main__:eval_samples_per_second: 4.321\n",
            "INFO:__main__:eval_steps_per_second: 0.278\n",
            "INFO:__main__:epoch: 0.1323918799646955\n",
            "INFO:__main__:\n",
            "Quick comparison training for T5...\n",
            "Map: 100%|█████████████████████████| 1811/1811 [00:00<00:00, 9257.27 examples/s]\n",
            "Map: 100%|███████████████████████████| 202/202 [00:00<00:00, 8275.37 examples/s]\n",
            "INFO:__main__:No checkpoint found. Starting from pretrained model.\n",
            "  0%|                                                    | 0/57 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
            "Traceback (most recent call last):\n",
            "  File \"/vol/bitbucket/ag724/individual_project/cah/personalized-humour-generation/cah-app/python-backend/src/model_comparison_reduced.py\", line 249, in <module>\n",
            "    main() \n",
            "    ^^^^^^\n",
            "  File \"/vol/bitbucket/ag724/individual_project/cah/personalized-humour-generation/cah-app/python-backend/src/model_comparison_reduced.py\", line 224, in main\n",
            "    trainer.train(resume_from_checkpoint=last_checkpoint)\n",
            "  File \"/vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages/transformers/trainer.py\", line 2245, in train\n",
            "    return inner_training_loop(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages/transformers/trainer.py\", line 2560, in _inner_training_loop\n",
            "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages/transformers/trainer.py\", line 3736, in training_step\n",
            "    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages/transformers/trainer.py\", line 3801, in compute_loss\n",
            "    outputs = model(**inputs)\n",
            "              ^^^^^^^^^^^^^^^\n",
            "  File \"/vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages/accelerate/utils/operations.py\", line 814, in forward\n",
            "    return model_forward(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages/accelerate/utils/operations.py\", line 802, in __call__\n",
            "    return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages/accelerate/utils/operations.py\", line 781, in convert_to_fp32\n",
            "    return recursively_apply(_convert_to_fp32, tensor, test_type=_is_fp16_bf16_tensor)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages/accelerate/utils/operations.py\", line 119, in recursively_apply\n",
            "    k: recursively_apply(\n",
            "       ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages/accelerate/utils/operations.py\", line 107, in recursively_apply\n",
            "    return honor_type(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages/accelerate/utils/operations.py\", line 81, in honor_type\n",
            "    return type(obj)(generator)\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages/accelerate/utils/operations.py\", line 110, in <genexpr>\n",
            "    recursively_apply(\n",
            "  File \"/vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages/accelerate/utils/operations.py\", line 107, in recursively_apply\n",
            "    return honor_type(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages/accelerate/utils/operations.py\", line 81, in honor_type\n",
            "    return type(obj)(generator)\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages/accelerate/utils/operations.py\", line 110, in <genexpr>\n",
            "    recursively_apply(\n",
            "  File \"/vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages/accelerate/utils/operations.py\", line 126, in recursively_apply\n",
            "    return func(data, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages/accelerate/utils/operations.py\", line 773, in _convert_to_fp32\n",
            "    return tensor.float()\n",
            "           ^^^^^^^^^^^^^^\n",
            "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.49 GiB of which 12.69 MiB is free. Process 2475292 has 224.00 MiB memory in use. Including non-PyTorch memory, this process has 23.24 GiB memory in use. Of the allocated memory 22.82 GiB is allocated by PyTorch, and 139.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "  0%|          | 0/57 [00:01<?, ?it/s]                                          \n"
          ]
        }
      ],
      "source": [
        "!python src/model_comparison_reduced.py --model t5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "usage: ipykernel_launcher.py [-h] [--model {t5,bart}]\n",
            "ipykernel_launcher.py: error: unrecognized arguments: -f /homes/ag724/.local/share/jupyter/runtime/kernel-4e9b5370-10f9-41a7-988b-564a91116428.json\n"
          ]
        },
        {
          "ename": "SystemExit",
          "evalue": "2",
          "output_type": "error",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Set memory options\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128,expandable_segments:True,garbage_collection_threshold:0.8\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "def clear_gpu_memory():\n",
        "    \"\"\"Clear GPU memory cache thoroughly.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        torch.cuda.synchronize()\n",
        "        logger.info(f\"GPU memory cleared. Current usage: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
        "\n",
        "def log_memory():\n",
        "    \"\"\"Log current GPU memory usage.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        allocated = torch.cuda.memory_allocated() / 1e9\n",
        "        reserved = torch.cuda.memory_reserved() / 1e9\n",
        "        logger.info(f\"GPU memory: allocated={allocated:.2f} GB, reserved={reserved:.2f} GB\")\n",
        "\n",
        "def run_inference(model_name):\n",
        "    \"\"\"Run simple inference with the model.\"\"\"\n",
        "    logger.info(f\"Running inference with {model_name}\")\n",
        "    \n",
        "    # Make sure memory is cleared before starting\n",
        "    clear_gpu_memory()\n",
        "    log_memory()\n",
        "    \n",
        "    try:\n",
        "        # Fixed sample inputs\n",
        "        sample_text = \"Complete this sentence: Cards Against Humanity is\"\n",
        "        \n",
        "        # Load tokenizer on CPU\n",
        "        logger.info(\"Loading tokenizer...\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        \n",
        "        # Load model with minimal memory usage\n",
        "        logger.info(\"Loading model...\")\n",
        "        \n",
        "        # Use CPU if GPU is not available or has very little memory\n",
        "        if not torch.cuda.is_available() or torch.cuda.memory_allocated() / torch.cuda.get_device_properties(0).total_memory > 0.8:\n",
        "            logger.info(\"Using CPU for inference\")\n",
        "            device = \"cpu\"\n",
        "        else:\n",
        "            logger.info(\"Using GPU for inference\")\n",
        "            device = \"cuda\"\n",
        "            \n",
        "        try:\n",
        "            if device == \"cuda\":\n",
        "                # Try loading with FP16 precision\n",
        "                model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "                    model_name,\n",
        "                    torch_dtype=torch.float16,\n",
        "                    device_map=\"auto\",\n",
        "                    low_cpu_mem_usage=True\n",
        "                )\n",
        "            else:\n",
        "                # Load on CPU\n",
        "                model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "                \n",
        "            # Log memory after loading\n",
        "            log_memory()\n",
        "            \n",
        "            # Tokenize input\n",
        "            inputs = tokenizer(sample_text, return_tensors=\"pt\")\n",
        "            if device == \"cuda\":\n",
        "                inputs = {k: v.cuda() for k, v in inputs.items()}\n",
        "            \n",
        "            # Run inference\n",
        "            logger.info(\"Running generation...\")\n",
        "            with torch.no_grad():\n",
        "                # Use minimal generation parameters\n",
        "                output_ids = model.generate(\n",
        "                    **inputs,\n",
        "                    max_length=30,\n",
        "                    num_beams=1,\n",
        "                    no_repeat_ngram_size=2,\n",
        "                    early_stopping=True\n",
        "                )\n",
        "            \n",
        "            # Convert to text\n",
        "            output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "            logger.info(f\"Input: {sample_text}\")\n",
        "            logger.info(f\"Output: {output_text}\")\n",
        "            \n",
        "            return {\n",
        "                \"model\": model_name,\n",
        "                \"input\": sample_text,\n",
        "                \"output\": output_text\n",
        "            }\n",
        "            \n",
        "        finally:\n",
        "            # Clean up\n",
        "            if 'model' in locals():\n",
        "                del model\n",
        "            if 'inputs' in locals():\n",
        "                del inputs\n",
        "            if 'output_ids' in locals():\n",
        "                del output_ids\n",
        "            clear_gpu_memory()\n",
        "            \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during inference: {str(e)}\")\n",
        "        return {\"model\": model_name, \"error\": str(e)}\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"Run minimal inference with a model\")\n",
        "    parser.add_argument(\"--model\", type=str, choices=[\"t5\", \"bart\"], default=\"t5\",\n",
        "                      help=\"Model to use for inference (t5 or bart)\")\n",
        "    args = parser.parse_args()\n",
        "    \n",
        "    # Define model mapping\n",
        "    model_map = {\n",
        "        \"t5\": \"t5-small\",\n",
        "        \"bart\": \"facebook/bart-base\"\n",
        "    }\n",
        "    \n",
        "    # Print GPU info\n",
        "    if torch.cuda.is_available():\n",
        "        logger.info(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        logger.info(f\"Total GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "        logger.info(f\"Initial GPU memory usage: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
        "    else:\n",
        "        logger.warning(\"No GPU available, using CPU\")\n",
        "    \n",
        "    # Run inference\n",
        "    model_name = model_map[args.model]\n",
        "    result = run_inference(model_name)\n",
        "    \n",
        "    # Print final result\n",
        "    if \"error\" in result:\n",
        "        logger.error(f\"Inference failed for {result['model']}: {result['error']}\")\n",
        "    else:\n",
        "        logger.info(f\"Successful inference with {result['model']}:\")\n",
        "        logger.info(f\"Input: {result['input']}\")\n",
        "        logger.info(f\"Output: {result['output']}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Make sure memory is clean at start\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "    \n",
        "    main() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU memory allocated: 0.00 GB\n",
            "GPU memory reserved: 0.00 GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "# Delete all variables that might hold references\n",
        "for obj in dir():\n",
        "    if obj not in ['torch', 'gc'] and not obj.startswith('__'):\n",
        "        del globals()[obj]\n",
        "\n",
        "# Force CUDA operations to complete\n",
        "torch.cuda.synchronize()\n",
        "\n",
        "# Clear cache and run garbage collection\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Check memory status\n",
        "print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
        "print(f\"GPU memory reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'_oh'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgc\u001b[39;00m\n\u001b[32m      4\u001b[39m gc.collect()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m torch.cuda.memory_summary(device=\u001b[38;5;28;01mNone\u001b[39;00m, abbreviated=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages/IPython/core/displayhook.py:280\u001b[39m, in \u001b[36mDisplayHook.__call__\u001b[39m\u001b[34m(self, result)\u001b[39m\n\u001b[32m    278\u001b[39m \u001b[38;5;28mself\u001b[39m.write_output_prompt()\n\u001b[32m    279\u001b[39m format_dict, md_dict = \u001b[38;5;28mself\u001b[39m.compute_format_data(result)\n\u001b[32m--> \u001b[39m\u001b[32m280\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mupdate_user_ns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[38;5;28mself\u001b[39m.fill_exec_result(result)\n\u001b[32m    282\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m format_dict:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages/IPython/core/displayhook.py:214\u001b[39m, in \u001b[36mDisplayHook.update_user_ns\u001b[39m\u001b[34m(self, result)\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Update user_ns with various things like _, __, _1, etc.\"\"\"\u001b[39;00m\n\u001b[32m    213\u001b[39m \u001b[38;5;66;03m# Avoid recursive reference when displaying _oh/Out\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cache_size \u001b[38;5;129;01mand\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshell\u001b[49m\u001b[43m.\u001b[49m\u001b[43muser_ns\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m_oh\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[32m    215\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.shell.user_ns[\u001b[33m'\u001b[39m\u001b[33m_oh\u001b[39m\u001b[33m'\u001b[39m]) >= \u001b[38;5;28mself\u001b[39m.cache_size \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.do_full_cache:\n\u001b[32m    216\u001b[39m         \u001b[38;5;28mself\u001b[39m.cull_cache()\n",
            "\u001b[31mKeyError\u001b[39m: '_oh'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.memory_summary(device=None, abbreviated=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "!python src/model_comparison_reduced.py --model bart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:__main__:Loading data...\n",
            "INFO:__main__:Using 2013 examples for quick model comparison\n",
            "INFO:__main__:\n",
            "Quick comparison training for BART...\n",
            "Map: 100%|█████████████████████████| 1811/1811 [00:00<00:00, 8725.49 examples/s]\n",
            "Map: 100%|███████████████████████████| 202/202 [00:00<00:00, 8353.54 examples/s]\n",
            "INFO:__main__:Found checkpoint: models/bart-large-cnn/checkpoint-300\n",
            "/vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages/transformers/models/bart/configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n",
            "  warnings.warn(\n",
            "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n",
            "Warning: The following arguments do not match the ones in the `trainer_state.json` within the checkpoint directory: \n",
            "\tlogging_steps: 50 (from args) != 100 (from trainer_state.json)\n",
            "\tper_device_train_batch_size: 16 (from args) != 8 (from trainer_state.json)\n",
            "  0%|                                                   | 0/113 [00:00<?, ?it/s]There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n",
            "{'train_runtime': 0.2932, 'train_samples_per_second': 6176.395, 'train_steps_per_second': 385.385, 'train_loss': 0.0, 'epoch': 0.13}\n",
            "  0%|                                                   | 0/113 [00:00<?, ?it/s]\n",
            "100%|███████████████████████████████████████████| 13/13 [00:42<00:00,  3.35s/it]INFO:absl:Using default tokenizer.\n",
            "100%|███████████████████████████████████████████| 13/13 [00:46<00:00,  3.60s/it]\n",
            "INFO:__main__:BART Results:\n",
            "INFO:__main__:eval_loss: 0.026430455967783928\n",
            "INFO:__main__:eval_rouge1: 29.4827\n",
            "INFO:__main__:eval_rouge2: 23.695\n",
            "INFO:__main__:eval_rougeL: 29.5668\n",
            "INFO:__main__:eval_rougeLsum: 29.482\n",
            "INFO:__main__:eval_runtime: 54.7882\n",
            "INFO:__main__:eval_samples_per_second: 3.687\n",
            "INFO:__main__:eval_steps_per_second: 0.237\n",
            "INFO:__main__:epoch: 0.1323918799646955\n",
            "INFO:__main__:\n",
            "Quick comparison training for T5...\n",
            "spiece.model: 100%|███████████████████████████| 792k/792k [00:00<00:00, 882kB/s]\n",
            "Map: 100%|█████████████████████████| 1811/1811 [00:00<00:00, 9410.93 examples/s]\n",
            "Map: 100%|███████████████████████████| 202/202 [00:00<00:00, 8347.37 examples/s]\n",
            "INFO:__main__:No checkpoint found. Starting from pretrained model.\n",
            "  0%|                                                    | 0/57 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
            "Traceback (most recent call last):\n",
            "  File \"/vol/bitbucket/ag724/individual_project/cah/personalized-humour-generation/cah-app/python-backend/src/model_comparison_reduced.py\", line 249, in <module>\n",
            "    main() \n",
            "    ^^^^^^\n",
            "  File \"/vol/bitbucket/ag724/individual_project/cah/personalized-humour-generation/cah-app/python-backend/src/model_comparison_reduced.py\", line 224, in main\n",
            "    trainer.train(resume_from_checkpoint=last_checkpoint)\n",
            "  File \"/vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages/transformers/trainer.py\", line 2245, in train\n",
            "    return inner_training_loop(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages/transformers/trainer.py\", line 2560, in _inner_training_loop\n",
            "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages/transformers/trainer.py\", line 3736, in training_step\n",
            "    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages/transformers/trainer.py\", line 3801, in compute_loss\n",
            "    outputs = model(**inputs)\n",
            "              ^^^^^^^^^^^^^^^\n",
            "  File \"/vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages/accelerate/utils/operations.py\", line 814, in forward\n",
            "    return model_forward(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages/accelerate/utils/operations.py\", line 802, in __call__\n",
            "    return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages/accelerate/utils/operations.py\", line 781, in convert_to_fp32\n",
            "    return recursively_apply(_convert_to_fp32, tensor, test_type=_is_fp16_bf16_tensor)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages/accelerate/utils/operations.py\", line 119, in recursively_apply\n",
            "    k: recursively_apply(\n",
            "       ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages/accelerate/utils/operations.py\", line 107, in recursively_apply\n",
            "    return honor_type(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages/accelerate/utils/operations.py\", line 81, in honor_type\n",
            "    return type(obj)(generator)\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages/accelerate/utils/operations.py\", line 110, in <genexpr>\n",
            "    recursively_apply(\n",
            "  File \"/vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages/accelerate/utils/operations.py\", line 107, in recursively_apply\n",
            "    return honor_type(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages/accelerate/utils/operations.py\", line 81, in honor_type\n",
            "    return type(obj)(generator)\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages/accelerate/utils/operations.py\", line 110, in <genexpr>\n",
            "    recursively_apply(\n",
            "  File \"/vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages/accelerate/utils/operations.py\", line 126, in recursively_apply\n",
            "    return func(data, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/vol/bitbucket/ag724/cahvenv/lib/python3.12/site-packages/accelerate/utils/operations.py\", line 773, in _convert_to_fp32\n",
            "    return tensor.float()\n",
            "           ^^^^^^^^^^^^^^\n",
            "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.49 GiB of which 2.25 MiB is free. Including non-PyTorch memory, this process has 23.47 GiB memory in use. Of the allocated memory 23.01 GiB is allocated by PyTorch, and 187.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "  0%|          | 0/57 [00:02<?, ?it/s]                                          \n"
          ]
        }
      ],
      "source": [
        "!python src/model_comparison_reduced.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python src/model_comparison_reduced.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU memory allocated: 0.00 GB\n",
            "GPU memory reserved: 0.00 GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "# Delete all variables that might hold references\n",
        "for obj in dir():\n",
        "    if obj not in ['torch', 'gc'] and not obj.startswith('__'):\n",
        "        del globals()[obj]\n",
        "\n",
        "# Force CUDA operations to complete\n",
        "torch.cuda.synchronize()\n",
        "\n",
        "# Clear cache and run garbage collection\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Check memory status\n",
        "print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
        "print(f\"GPU memory reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: True\n",
            "Current device: 0\n",
            "Device name: NVIDIA A30\n",
            "Allocated memory (MB): 0.0\n",
            "Reserved memory (MB): 0.0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Current device:\", torch.cuda.current_device())\n",
        "    print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
        "    print(\"Allocated memory (MB):\", torch.cuda.memory_allocated() / 1024**2)\n",
        "    print(\"Reserved memory (MB):\", torch.cuda.memory_reserved() / 1024**2)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
