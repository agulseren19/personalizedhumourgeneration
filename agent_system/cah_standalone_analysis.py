#!/usr/bin/env python3
"""
Standalone CAH Analysis - Baseline vs Multi-Agent Comparison
Runs without complex import dependencies
"""

import json
import os
import asyncio
import time
import random
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Try to import LLM clients
try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

try:
    from anthropic import Anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False

@dataclass
class CAHCard:
    text: str
    card_type: str  # 'black' or 'white'
    pick_count: int = 1  # How many white cards to pick

@dataclass
class CAHCombination:
    black_card: str
    white_cards: List[str]
    result: str
    score: float = 0.0

class SimpleCAHDataset:
    """Simple CAH dataset with sample cards"""
    
    def __init__(self):
        self.black_cards = [
            CAHCard("What's the next Happy Meal toy? _____.", "black", 1),
            CAHCard("I never truly understood _____ until I encountered _____.", "black", 2),
            CAHCard("What's that smell? _____.", "black", 1),
            CAHCard("Life for American Indians was forever changed when the White Man introduced them to _____.", "black", 1),
            CAHCard("_____ + _____ = _____.", "black", 3),
            CAHCard("What's a girl's best friend? _____.", "black", 1),
            CAHCard("What did I bring back from Mexico? _____.", "black", 1),
            CAHCard("What's the most emo? _____.", "black", 1),
            CAHCard("What would grandma find disturbing, yet oddly charming? _____.", "black", 1),
            CAHCard("What helps Obama unwind? _____.", "black", 1),
        ]
        
        self.white_cards = [
            "Being rich",
            "Soup that is too hot", 
            "A sad handjob",
            "The Heart of Darkness",
            "Vigorous jazz hands",
            "Finger painting",
            "A disappointing birthday party",
            "My collection of high-tech sex toys",
            "Passive-aggressive Post-it notes",
            "The inevitable heat death of the universe",
            "A windmill full of corpses",
            "Pretending to care",
            "The chronic",
            "Bees?",
            "Dead parents",
            "Alcoholism",
            "Coat hanger abortions",
            "Surprise sex!",
            "The Big Bang",
            "Racism",
        ]
    
    def get_random_combination(self) -> CAHCombination:
        """Get a random black/white card combination"""
        black_card = random.choice(self.black_cards)
        white_cards = random.sample(self.white_cards, black_card.pick_count)
        
        # Create the result by filling in blanks
        result = black_card.text
        for white_card in white_cards:
            result = result.replace("_____", white_card, 1)
        
        return CAHCombination(
            black_card=black_card.text,
            white_cards=white_cards,
            result=result
        )

class SimpleLLMClient:
    """Simple LLM client that handles multiple providers"""
    
    def __init__(self):
        self.openai_client = None
        self.anthropic_client = None
        
        # Initialize clients if API keys are available
        if OPENAI_AVAILABLE and os.getenv("OPENAI_API_KEY"):
            self.openai_client = openai.AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        
        if ANTHROPIC_AVAILABLE and os.getenv("ANTHROPIC_API_KEY"):
            self.anthropic_client = Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
    
    async def generate_response(self, prompt: str, model: str = "gpt-3.5-turbo", temperature: float = 0.8) -> str:
        """Generate response using available LLM"""
        try:
            if model.startswith("gpt") and self.openai_client:
                return await self._generate_openai(prompt, model, temperature)
            elif model.startswith("claude") and self.anthropic_client:
                return await self._generate_anthropic(prompt, model, temperature)
            else:
                # Fallback to mock response
                return self._generate_mock(prompt)
        except Exception as e:
            print(f"âš ï¸  LLM generation failed: {e}")
            return self._generate_mock(prompt)
    
    async def _generate_openai(self, prompt: str, model: str, temperature: float) -> str:
        """Generate using OpenAI"""
        response = await self.openai_client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=temperature,
            max_tokens=200
        )
        return response.choices[0].message.content.strip()
    
    async def _generate_anthropic(self, prompt: str, model: str, temperature: float) -> str:
        """Generate using Anthropic (sync client)"""
        # Note: Anthropic client is sync, so we run it in executor
        def _sync_generate():
            response = self.anthropic_client.messages.create(
                model=model,
                max_tokens=200,
                temperature=temperature,
                messages=[{"role": "user", "content": prompt}]
            )
            return response.content[0].text.strip()
        
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, _sync_generate)
    
    def _generate_mock(self, prompt: str) -> str:
        """Generate mock response when no LLM is available"""
        mock_responses = [
            "My collection of high-tech sex toys",
            "Passive-aggressive Post-it notes", 
            "The inevitable heat death of the universe",
            "Bees?",
            "Dead parents",
            "Alcoholism"
        ]
        return random.choice(mock_responses)

class CAHHumorEvaluator:
    """Evaluates humor quality of CAH combinations"""
    
    def __init__(self, llm_client: SimpleLLMClient):
        self.llm_client = llm_client
    
    async def evaluate_humor(self, combination: CAHCombination, model: str = "gpt-3.5-turbo") -> float:
        """Evaluate humor on a scale of 0-1"""
        prompt = f"""
Rate the humor quality of this Cards Against Humanity combination on a scale of 0.0 to 1.0:

Black Card: {combination.black_card}
White Card(s): {', '.join(combination.white_cards)}
Result: {combination.result}

Consider:
- Unexpectedness and surprise
- Cleverness of the combination
- Comedic timing and flow
- Appropriateness for CAH's edgy humor style

Respond with just a number between 0.0 and 1.0.
"""
        
        try:
            response = await self.llm_client.generate_response(prompt, model, temperature=0.3)
            # Extract number from response
            score = float(response.strip())
            return max(0.0, min(1.0, score))  # Clamp to 0-1 range
        except:
            # Fallback to random score
            return random.uniform(0.3, 0.8)

class BaselineCAHGenerator:
    """Simple baseline CAH generator"""
    
    def __init__(self, llm_client: SimpleLLMClient):
        self.llm_client = llm_client
    
    async def generate_white_card(self, black_card: str, model: str = "gpt-3.5-turbo") -> str:
        """Generate a white card response for a black card"""
        prompt = f"""
Generate a funny white card response for this Cards Against Humanity black card:

{black_card}

The response should be:
- Unexpected and surprising
- Edgy but not extremely offensive
- Fits the fill-in-the-blank format
- Short and punchy (1-8 words)

Respond with just the white card text, no quotes or explanation.
"""
        
        return await self.llm_client.generate_response(prompt, model, temperature=0.9)

class MultiAgentCAHGenerator:
    """Multi-agent CAH generator with candidate generation and evaluation"""
    
    def __init__(self, llm_client: SimpleLLMClient, evaluator: CAHHumorEvaluator):
        self.llm_client = llm_client
        self.evaluator = evaluator
    
    async def generate_white_card(self, black_card: str, model: str = "gpt-3.5-turbo") -> str:
        """Generate white card using multi-agent approach"""
        
        # Step 1: Generate multiple candidates
        candidates = await self._generate_candidates(black_card, model, num_candidates=5)
        
        # Step 2: Evaluate each candidate
        evaluated_candidates = []
        for candidate in candidates:
            combination = CAHCombination(
                black_card=black_card,
                white_cards=[candidate],
                result=black_card.replace("_____", candidate, 1)
            )
            score = await self.evaluator.evaluate_humor(combination, model)
            evaluated_candidates.append((candidate, score))
        
        # Step 3: Select best candidate
        best_candidate = max(evaluated_candidates, key=lambda x: x[1])
        
        # Step 4: Refine the best candidate
        refined = await self._refine_candidate(black_card, best_candidate[0], model)
        
        return refined
    
    async def _generate_candidates(self, black_card: str, model: str, num_candidates: int) -> List[str]:
        """Generate multiple candidate responses"""
        prompt = f"""
Generate {num_candidates} different funny white card responses for this Cards Against Humanity black card:

{black_card}

Each response should be:
- Unexpected and surprising
- Edgy but not extremely offensive  
- Fits the fill-in-the-blank format
- Short and punchy (1-8 words)
- Different from the others

Format as a numbered list:
1. [response 1]
2. [response 2]
...
"""
        
        response = await self.llm_client.generate_response(prompt, model, temperature=1.0)
        
        # Parse the numbered list
        candidates = []
        for line in response.split('\n'):
            if line.strip() and any(line.strip().startswith(f"{i}.") for i in range(1, num_candidates + 1)):
                candidate = line.split('.', 1)[1].strip()
                if candidate:
                    candidates.append(candidate)
        
        # Fallback if parsing fails
        if not candidates:
            candidates = [response.strip()]
        
        return candidates[:num_candidates]
    
    async def _refine_candidate(self, black_card: str, candidate: str, model: str) -> str:
        """Refine the selected candidate"""
        prompt = f"""
Refine this Cards Against Humanity white card response to make it funnier:

Black Card: {black_card}
Current White Card: {candidate}
Current Result: {black_card.replace("_____", candidate, 1)}

Make it:
- More unexpected or clever
- Better comedic timing
- More concise if possible
- Keep the same general concept but improve execution

Respond with just the improved white card text.
"""
        
        try:
            refined = await self.llm_client.generate_response(prompt, model, temperature=0.7)
            return refined.strip()
        except:
            return candidate  # Return original if refinement fails

async def run_cah_analysis():
    """Run the complete CAH analysis comparing baseline vs multi-agent"""
    
    print("ğŸ­ CARDS AGAINST HUMANITY - BASELINE vs MULTI-AGENT ANALYSIS")
    print("=" * 80)
    
    # Initialize components
    dataset = SimpleCAHDataset()
    llm_client = SimpleLLMClient()
    evaluator = CAHHumorEvaluator(llm_client)
    baseline_generator = BaselineCAHGenerator(llm_client)
    multiagent_generator = MultiAgentCAHGenerator(llm_client, evaluator)
    
    # Check LLM availability
    has_real_llm = (llm_client.openai_client is not None or 
                    llm_client.anthropic_client is not None)
    
    if has_real_llm:
        print("âœ… Real LLM clients available")
        model = "gpt-3.5-turbo" if llm_client.openai_client else "claude-3-haiku"
    else:
        print("âš ï¸  No LLM clients available - running in demo mode")
        model = "mock"
    
    print(f"ğŸ¤– Using model: {model}")
    print()
    
    # Run analysis on multiple combinations
    num_tests = 5
    baseline_scores = []
    multiagent_scores = []
    
    print(f"ğŸ§ª Running analysis on {num_tests} combinations...")
    print()
    
    for i in range(num_tests):
        print(f"Test {i+1}/{num_tests}")
        print("-" * 40)
        
        # Get random combination
        combination = dataset.get_random_combination()
        black_card = combination.black_card
        
        print(f"Black Card: {black_card}")
        
        # Baseline generation
        print("ğŸ¯ Baseline generation...")
        baseline_white = await baseline_generator.generate_white_card(black_card, model)
        baseline_result = black_card.replace("_____", baseline_white, 1)
        baseline_combination = CAHCombination(
            black_card=black_card,
            white_cards=[baseline_white],
            result=baseline_result
        )
        baseline_score = await evaluator.evaluate_humor(baseline_combination, model)
        baseline_scores.append(baseline_score)
        
        print(f"   White Card: {baseline_white}")
        print(f"   Result: {baseline_result}")
        print(f"   Score: {baseline_score:.3f}")
        
        # Multi-agent generation
        print("ğŸ¤– Multi-agent generation...")
        multiagent_white = await multiagent_generator.generate_white_card(black_card, model)
        multiagent_result = black_card.replace("_____", multiagent_white, 1)
        multiagent_combination = CAHCombination(
            black_card=black_card,
            white_cards=[multiagent_white],
            result=multiagent_result
        )
        multiagent_score = await evaluator.evaluate_humor(multiagent_combination, model)
        multiagent_scores.append(multiagent_score)
        
        print(f"   White Card: {multiagent_white}")
        print(f"   Result: {multiagent_result}")
        print(f"   Score: {multiagent_score:.3f}")
        
        improvement = ((multiagent_score - baseline_score) / baseline_score * 100) if baseline_score > 0 else 0
        print(f"   Improvement: {improvement:+.1f}%")
        print()
    
    # Calculate final results
    avg_baseline = sum(baseline_scores) / len(baseline_scores)
    avg_multiagent = sum(multiagent_scores) / len(multiagent_scores)
    overall_improvement = ((avg_multiagent - avg_baseline) / avg_baseline * 100) if avg_baseline > 0 else 0
    
    print("=" * 80)
    print("ğŸ“Š FINAL RESULTS")
    print("=" * 80)
    print(f"ğŸ¯ Baseline Average Score: {avg_baseline:.3f}")
    print(f"ğŸ¤– Multi-Agent Average Score: {avg_multiagent:.3f}")
    print(f"ğŸ“ˆ Overall Improvement: {overall_improvement:+.1f}%")
    print()
    
    # Save results
    results = {
        "model_used": model,
        "has_real_llm": has_real_llm,
        "num_tests": num_tests,
        "baseline_scores": baseline_scores,
        "multiagent_scores": multiagent_scores,
        "avg_baseline_score": avg_baseline,
        "avg_multiagent_score": avg_multiagent,
        "overall_improvement_percent": overall_improvement,
        "timestamp": time.time()
    }
    
    results_file = "cah_baseline_vs_multiagent_results.json"
    with open(results_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"ğŸ’¾ Results saved to: {results_file}")
    
    # Interpretation
    print()
    print("ğŸ” INTERPRETATION:")
    if overall_improvement > 5:
        print("âœ… Multi-agent approach shows significant improvement!")
    elif overall_improvement > 0:
        print("ğŸ“ˆ Multi-agent approach shows modest improvement.")
    else:
        print("ğŸ“Š Results are mixed - may need more testing or refinement.")
    
    if not has_real_llm:
        print("âš ï¸  Note: Results are from demo mode. Use real LLMs for accurate comparison.")

if __name__ == "__main__":
    asyncio.run(run_cah_analysis()) 